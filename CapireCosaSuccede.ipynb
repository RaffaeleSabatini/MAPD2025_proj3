{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80b60425-c731-4591-9166-8d40b259e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68117f4f-8740-4a12-a947-4eceb6fa3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 Docker, 1 CloudVeneto\n",
    "FIGHTER = 1\n",
    "core = 4\n",
    "nome = \"marco\"\n",
    "#nicolò\n",
    "#marco\n",
    "#francesco\n",
    "#raffaele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25d6fccc-e688-44c9-8812-a64d08b0d9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hai settato 4041 e user_b: tu sei marco\n"
     ]
    }
   ],
   "source": [
    "# mapping utenti → (SparkUI, user)\n",
    "mappa = {\n",
    "    \"nicolò\":   (4040, \"user_a\"),\n",
    "    \"marco\":    (4041, \"user_b\"),\n",
    "    \"francesco\":(4042, \"user_c\"),\n",
    "    \"raffaele\": (4043, \"user_d\"),\n",
    "}\n",
    "# recupero SparkUI e user dal mapping\n",
    "SparkUI, user = mappa[nome]\n",
    "Npartition = 4*core # Regola generale: numero_partizioni = numero_core * (2 o 4)\n",
    "\n",
    "print(f\"Hai settato {SparkUI} e {user}: tu sei {nome}\")\n",
    "Npartition = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cf7b138-5627-496f-96c2-a1418066309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, BooleanType, StringType\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    coalesce, corr, array,\n",
    "    col, lit, expr, when, count, count_if, row_number, sum as spark_sum, abs as spark_abs,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, lead, row_number, desc, asc, bool_or, floor,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1a2db8e-b755-4e76-b608-f69aaee7606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIGHTER==0:\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .appName(\"ProjectDocker\") \\\n",
    "        .config(\"spark.executor.memory\", \"1000m\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "        # SE NON FUNZIONA TOGLI I DUE CONFIG DI ARROW\n",
    "\n",
    "        # .config(\"spark.executor.memory\", \"1500m\")\n",
    "        # .config(\"spark.executor.cores\", \"1\")\\\n",
    "        # .config(\"spark.executor.instances\", \"12\")\\\n",
    "        # .config(\"spark.cores.max\", \"12\")\\\n",
    "        # .config(\"spark.default.parallelism\", \"24\")\\\n",
    "        # .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "\n",
    "elif FIGHTER==1:\n",
    "\n",
    "        os.environ[\"PYSPARK_PYTHON\"] = \"/opt/miniconda3/bin/python\"\n",
    "        os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/miniconda3/bin/python\"\n",
    "        \n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ProjectCloudVeneto\") \\\n",
    "            .master(\"spark://10.67.22.135:7077\") \\\n",
    "            .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "            .config(\"spark.scheduler.pool\", user) \\\n",
    "            .config(\"spark.scheduler.allocation.file\", \"file:///usr/local/spark/conf/fairscheduler.xml\") \\\n",
    "            .config(\"spark.cores.max\", core) \\\n",
    "            .config(\"spark.executor.memory\", \"800m\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "            .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "            .config(\"spark.ui.port\", SparkUI) \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "else : print(\"Better choose an available fighter, you little bastard.\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba300c4c-c6b1-45e1-98bc-06eed8ea5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if FIGHTER==0:\n",
    "#     df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/ProvePreliminari/SW-106.csv\")\n",
    "\n",
    "# elif FIGHTER==1:\n",
    "#     df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Better choose an available fighter, you little bastard\")\n",
    "\n",
    "# df = df.repartition(Npartition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a205d-1fc4-42a1-9ad0-5d0a019f1446",
   "metadata": {},
   "source": [
    "# Unixtime (utile per controllare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12afd444-d76d-4f90-a78b-e642ebad9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01 00:00:30\n",
      "2020-10-01 00:02:30\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print ( datetime.fromtimestamp(1601510430) )\n",
    "print ( datetime.fromtimestamp(1601510550) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e1ef9-0a78-4112-af3f-6fec1d3cbd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634333cb-c423-4490-bcc8-cdc53ace6dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf0f07d0-b73e-4043-9c8b-639e1085b2f3",
   "metadata": {},
   "source": [
    "# expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3dc375b-74a1-4e30-b3b7-04236992d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame iniziale:\n",
      "+-----------+---+---+\n",
      "|time_window| A5| A7|\n",
      "+-----------+---+---+\n",
      "|    window1|  1| 10|\n",
      "|    window1|  0| 20|\n",
      "|    window2|  1| 30|\n",
      "|    window2|  1| 40|\n",
      "+-----------+---+---+\n",
      "\n",
      "+--------+\n",
      "|media A7|\n",
      "+--------+\n",
      "|    25.0|\n",
      "+--------+\n",
      "\n",
      "+-----------+--------+\n",
      "|time_window|media A7|\n",
      "+-----------+--------+\n",
      "|    window1|    15.0|\n",
      "|    window2|    35.0|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"window1\", 1, 10),\n",
    "    (\"window1\", 0, 20),\n",
    "    (\"window2\", 1, 30),\n",
    "    (\"window2\", 1, 40),\n",
    "]\n",
    "columns = [\"time_window\", \"A5\", \"A7\"]\n",
    "\n",
    "df_1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"DataFrame iniziale:\")\n",
    "df_1.show()\n",
    "\n",
    "\n",
    "df_2 = df_1.agg(spark_avg(col(\"A7\")).alias(\"media A7\"))  #di fatto sottointeso .goupBy su tutto il dataframe\n",
    "df_2.show()\n",
    "\n",
    "\n",
    "df_3 = df_1.groupBy(\"time_window\").agg(spark_avg(col(\"A7\")).alias(\"media A7\"))\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a15d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096037e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a02068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249fa25a-8720-4a82-99bf-7952861e8db9",
   "metadata": {},
   "source": [
    "# agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5077fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame iniziale:\n",
      "+-----------+---+---+\n",
      "|time_window| A5| A7|\n",
      "+-----------+---+---+\n",
      "|    window1|  1| 10|\n",
      "|    window1|  0| 20|\n",
      "|    window2|  1| 30|\n",
      "|    window2|  1| 40|\n",
      "+-----------+---+---+\n",
      "\n",
      "Risultato aggregato:\n",
      "+-----------+---+----+\n",
      "|time_window| A5|  A7|\n",
      "+-----------+---+----+\n",
      "|    window2|  1|35.0|\n",
      "|    window1|  1|15.0|\n",
      "+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Creare il DataFrame di esempio\n",
    "data = [\n",
    "    (\"window1\", 1, 10),\n",
    "    (\"window1\", 0, 20),\n",
    "    (\"window2\", 1, 30),\n",
    "    (\"window2\", 1, 40),\n",
    "]\n",
    "columns = [\"time_window\", \"A5\", \"A7\"]\n",
    "\n",
    "df_windowed = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"DataFrame iniziale:\")\n",
    "df_windowed.show()\n",
    "\n",
    "# 3. Definire le aggregazioni\n",
    "aggs = [\n",
    "    spark_max(col(\"A5\")).alias(\"A5\"),   # MAX per A5\n",
    "    spark_avg(col(\"A7\")).alias(\"A7\")    # AVG per A7\n",
    "]\n",
    "\n",
    "# 4. Eseguire la groupBy con le aggregazioni\n",
    "result_df = df_windowed.groupBy(\"time_window\").agg(*aggs)\n",
    "\n",
    "print(\"Risultato aggregato:\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d198ac8-380a-4684-8b6c-d3d9913295b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85f95f-a1a1-4016-933c-dbea25403fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48927d07-fae4-4b3a-8559-ddaaeca72085",
   "metadata": {},
   "source": [
    "# window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c31bdb-eb47-45a2-8538-b461f55a7ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame originale:\n",
      "+-----+-----+-------------------+\n",
      "|user |value|timestamp          |\n",
      "+-----+-----+-------------------+\n",
      "|user1|10   |2023-01-01 12:00:00|\n",
      "|user2|20   |2023-01-01 12:03:00|\n",
      "|user1|30   |2023-01-01 12:07:00|\n",
      "|user2|40   |2023-01-01 12:12:00|\n",
      "|user1|50   |2023-01-01 12:18:00|\n",
      "+-----+-----+-------------------+\n",
      "\n",
      "Aggregato per finestre di 10 minuti:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |avg_value|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-01-01 12:00:00, 2023-01-01 12:10:00}|20.0     |3    |\n",
      "|{2023-01-01 12:10:00, 2023-01-01 12:20:00}|45.0     |2    |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. Creiamo un DataFrame con timestamp + valori\n",
    "data = [\n",
    "    (\"user1\", 10, datetime.datetime(2023, 1, 1, 12, 0, 0)),\n",
    "    (\"user2\", 20, datetime.datetime(2023, 1, 1, 12, 3, 0)),\n",
    "    (\"user1\", 30, datetime.datetime(2023, 1, 1, 12, 7, 0)),\n",
    "    (\"user2\", 40, datetime.datetime(2023, 1, 1, 12, 12, 0)),\n",
    "    (\"user1\", 50, datetime.datetime(2023, 1, 1, 12, 18, 0)),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"DataFrame originale:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# 3. Applichiamo una window di 10 minuti sul timestamp\n",
    "df_windowed = (\n",
    "    df.groupBy(window(col(\"timestamp\"), \"10 minutes\"))  # ogni 10 minuti\n",
    "      .agg(\n",
    "          spark_avg(\"value\").alias(\"avg_value\"),\n",
    "          count(\"*\").alias(\"count\")\n",
    "      )\n",
    "      .orderBy(\"window\")\n",
    ")\n",
    "\n",
    "print(\"Aggregato per finestre di 10 minuti:\")\n",
    "df_windowed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52c85e75-06c5-4984-981a-24c7f4573b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame originale:\n",
      "+---+-------------------+\n",
      "|id |timestamp          |\n",
      "+---+-------------------+\n",
      "|1  |2023-01-01 12:00:00|\n",
      "|2  |2023-01-01 12:03:00|\n",
      "|3  |2023-01-01 12:07:00|\n",
      "|4  |2023-01-01 12:12:00|\n",
      "+---+-------------------+\n",
      "\n",
      "DataFrame con le finestre temporali:\n",
      "+------------------------------------------+\n",
      "|time_window                               |\n",
      "+------------------------------------------+\n",
      "|{2023-01-01 12:00:00, 2023-01-01 12:10:00}|\n",
      "|{2023-01-01 12:00:00, 2023-01-01 12:10:00}|\n",
      "|{2023-01-01 12:00:00, 2023-01-01 12:10:00}|\n",
      "|{2023-01-01 12:10:00, 2023-01-01 12:20:00}|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Piccolo dataset con timestamp\n",
    "data = [\n",
    "    (1, datetime.datetime(2023, 1, 1, 12, 0, 0)),\n",
    "    (2, datetime.datetime(2023, 1, 1, 12, 3, 0)),\n",
    "    (3, datetime.datetime(2023, 1, 1, 12, 7, 0)),\n",
    "    (4, datetime.datetime(2023, 1, 1, 12, 12, 0)),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"DataFrame originale:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# 3. Usiamo SOLO la funzione window\n",
    "df_windowed = df.select(window(col(\"timestamp\"), \"10 minutes\").alias(\"time_window\"))\n",
    "\n",
    "print(\"DataFrame con le finestre temporali:\")\n",
    "df_windowed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f24fec-fb4c-43f0-8d35-19e7f845bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {2023-01-01 12:00:00, 2023-01-01 12:10:00}\n",
    "# in realtà è una struct con due campi: start e end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6bdbb6-e554-40bb-a410-e00ba87a7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|             inizio|               fine|\n",
      "+-------------------+-------------------+\n",
      "|2023-01-01 12:00:00|2023-01-01 12:10:00|\n",
      "|2023-01-01 12:00:00|2023-01-01 12:10:00|\n",
      "|2023-01-01 12:00:00|2023-01-01 12:10:00|\n",
      "|2023-01-01 12:10:00|2023-01-01 12:20:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estremi = df_windowed.select(col(\"time_window.start\").alias(\"inizio\"),col(\"time_window.end\").alias(\"fine\"))\n",
    "df_estremi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f5d62-3b4e-487c-8091-07291c7557bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4dcd717-dbe6-4f88-b656-3b3013216d89",
   "metadata": {},
   "source": [
    "# lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df5bcf-b338-41c3-af4c-f0e140ee8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit(1) → crea una colonna con il valore costante 1 in tutte le righe.\n",
    "# lit(\"ciao\") → colonna costante con \"ciao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "448c621f-d2a3-41c1-97ca-eeefca576ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"DataFrame originale:\")\n",
    "df.show()\n",
    "\n",
    "# Aggiungiamo una colonna costante con lit(1)\n",
    "df_with_flag = df.withColumn(\"flag-lit1-\", lit(1))\n",
    "df_prova = df_with_flag.withColumn(\"flag-ciao\", lit(\"ciao\"))\n",
    "\n",
    "df_prova.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30adea-01cd-4d19-ae1d-538b34162a42",
   "metadata": {},
   "source": [
    "# PartitonBy DI WINDOW (OCCHIO IMPORTANTE SIA DI WINDOW il partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d96769-6a60-4893-9b7d-07ddbbbb9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 10),\n",
    "    (\"Alice\", 20),\n",
    "    (\"Bob\", 5),\n",
    "    (\"Bob\", 15)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"score\"])\n",
    "print(\"DataFrame originale:\")\n",
    "df.show()\n",
    "\n",
    "# 1️⃣ Window globale usando lit(1)\n",
    "w_global = Window.partitionBy(lit(1)).orderBy(\"score\")\n",
    "df_global = df.withColumn(\"row_num_global\", row_number().over(w_global))\n",
    "\n",
    "print(\"Row number con partitionBy(lit(1)) (tutti in un unico gruppo):\")\n",
    "df_global.show()\n",
    "\n",
    "# 2️⃣ Window per gruppo usando name\n",
    "w_by_name = Window.partitionBy(\"name\").orderBy(\"score\")\n",
    "df_by_name = df.withColumn(\"row_num_by_name\", row_number().over(w_by_name))\n",
    "\n",
    "print(\"Row number con partitionBy('name') (numerazione separata per ogni name):\")\n",
    "df_by_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369d595-0139-4ce1-9ae0-28e0b7df299f",
   "metadata": {},
   "source": [
    "Come funziona realmente partitionBy in una Window\n",
    "\n",
    "Non sposta fisicamente i dati sul disco\n",
    "Quando scrivi\n",
    "\n",
    "w = Window.partitionBy(\"name\").orderBy(\"score\")\n",
    "df.withColumn(\"row_num\", row_number().over(w))\n",
    "\n",
    "Spark non riscrive fisicamente il DataFrame in partizioni distinte sul cluster.\n",
    "Internamente crea solo una logica di raggruppamento temporaneo in memoria per applicare le funzioni di finestra.\n",
    "È un espediente logico per le window functions\n",
    "Spark sa che per calcolare row_number, rank, lag, ecc., deve trattare insieme tutte le righe con lo stesso valore di partitionBy.\n",
    "Poi le ordina secondo orderBy all’interno di ciascuna “partizione logica” per calcolare la funzione.\n",
    "\n",
    "Quando il dati vengono fisicamente spostati? ---- QUESTO E' IL PARTITION BY DEL DATAFRAME, NON DI WINDOW -----\n",
    "Solo se fai operazioni come repartition(\"name\") o write.partitionBy(\"name\").\n",
    "In quel caso Spark crea partizioni fisiche distinte, cioè i dati vengono distribuiti su più nodi o file separati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372f589-f9ea-4a3a-b36a-7019153dcda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ee019e-4f24-499e-b377-0a2c39afadcb",
   "metadata": {},
   "source": [
    "# Break lineage (da approfondire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84f824-4103-4768-97e5-6262e6ef7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anom = spark.createDataFrame(df_anom.rdd, df_anom.schema)  # Break lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a86f9-1bc7-4aeb-a352-3ebfca5fc40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57913674-0b88-4810-9d46-d7a9b971b6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabcf658-66dd-4812-ac92-eeea5cebf1e9",
   "metadata": {},
   "source": [
    "# Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a87c46-4a24-43ba-a16e-6f1997fff051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDummyAnomaly(nBlocks, feature_name, values):\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"BlockID\", IntegerType(), True),\n",
    "        StructField(\"window_id\", IntegerType(), True),\n",
    "        StructField(\"when\", IntegerType(), True),\n",
    "        StructField(feature_name, IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for k in range(nBlocks):\n",
    "        for n in range(len(values)):\n",
    "            data.append((k, n, k*len(values)+n*60, values[n]))\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df.orderBy('BlockID','when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbd4780-ebac-4203-b734-b454cef87120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+----+\n",
      "|BlockID|window_id|when|S117|\n",
      "+-------+---------+----+----+\n",
      "|      0|        0|   0|   0|\n",
      "|      0|        1|  60|   0|\n",
      "|      0|        2| 120|   1|\n",
      "|      0|        3| 180|   1|\n",
      "|      0|        4| 240|   1|\n",
      "|      0|        5| 300|   0|\n",
      "|      0|        6| 360|   0|\n",
      "|      0|        7| 420|   0|\n",
      "|      0|        8| 480|   0|\n",
      "|      0|        9| 540|   1|\n",
      "|      0|       10| 600|   1|\n",
      "|      0|       11| 660|   0|\n",
      "|      0|       12| 720|   1|\n",
      "|      0|       13| 780|   0|\n",
      "|      0|       14| 840|   1|\n",
      "|      0|       15| 900|   0|\n",
      "|      0|       16| 960|   0|\n",
      "|      0|       17|1020|   0|\n",
      "|      0|       18|1080|   0|\n",
      "|      0|       19|1140|   1|\n",
      "|      0|       20|1200|   1|\n",
      "|      0|       21|1260|   1|\n",
      "|      0|       22|1320|   1|\n",
      "|      1|        0|  23|   0|\n",
      "|      1|        1|  83|   0|\n",
      "|      1|        2| 143|   1|\n",
      "|      1|        3| 203|   1|\n",
      "|      1|        4| 263|   1|\n",
      "|      1|        5| 323|   0|\n",
      "|      1|        6| 383|   0|\n",
      "+-------+---------+----+----+\n",
      "only showing top 30 rows\n",
      "\n",
      "numero righe  46\n"
     ]
    }
   ],
   "source": [
    "dummy_df = createDummyAnomaly(2, 'S117', [0,0,1,1,1,0,0,0,0,1,1,0,1,0,1,0,0,0,0,1,1,1,1])\n",
    "dummy_df.show(30)\n",
    "print( f\"numero righe  {dummy_df.count()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cf9a1-b952-416f-af5b-f7855ab712b4",
   "metadata": {},
   "source": [
    "# Lag / Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20b0132d-ffac-4d3e-96c3-3787e8119d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: <class 'pyspark.sql.window.WindowSpec'>\n",
      "Rappresentazione: <pyspark.sql.window.WindowSpec object at 0x7f6fc7e80280>\n",
      "+---+-----+-----+----------+\n",
      "| id|group|value|prev_value|\n",
      "+---+-----+-----+----------+\n",
      "|  1|    A|  100|       200|\n",
      "|  2|    A|  200|       300|\n",
      "|  3|    A|  300|      NULL|\n",
      "|  4|    B|  400|       500|\n",
      "|  5|    B|  500|       600|\n",
      "|  6|    B|  600|      NULL|\n",
      "+---+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creo un DataFrame di esempio\n",
    "data = [\n",
    "    (1, \"A\", 100),\n",
    "    (2, \"A\", 200),\n",
    "    (3, \"A\", 300),\n",
    "    (4, \"B\", 400),\n",
    "    (5, \"B\", 500),\n",
    "    (6, \"B\", 600),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"group\", \"value\"])\n",
    "\n",
    "# Definisco una finestra per \"group\" ordinata per \"id\"\n",
    "windowSpec = Window.partitionBy(\"group\").orderBy(\"id\")\n",
    "\n",
    "\n",
    "# Applico la funzione lag per ottenere il valore precedente\n",
    "df_with_lag = df.withColumn(\"prev_value\", lead(\"value\", 1).over(windowSpec)) # lag or lead --- cambia  qui\n",
    "\n",
    "# Stampo il tipo e la rappresentazione della windowSpec\n",
    "print(\"Tipo:\", type(windowSpec))\n",
    "print(\"Rappresentazione:\", windowSpec)\n",
    "\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0809e56d-d10f-4a3a-a329-7841adfdec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+----+-----------+\n",
      "|BlockID|window_id|when|S117|lagged_S117|\n",
      "+-------+---------+----+----+-----------+\n",
      "|      0|        0|   0|   0|       NULL|\n",
      "|      0|        1|  60|   0|          0|\n",
      "|      0|        2| 120|   1|          0|\n",
      "|      0|        3| 180|   1|          1|\n",
      "|      0|        4| 240|   1|          1|\n",
      "|      0|        5| 300|   0|          1|\n",
      "|      0|        6| 360|   0|          0|\n",
      "|      0|        7| 420|   0|          0|\n",
      "|      0|        8| 480|   0|          0|\n",
      "|      0|        9| 540|   1|          0|\n",
      "|      0|       10| 600|   1|          1|\n",
      "|      0|       11| 660|   0|          1|\n",
      "|      0|       12| 720|   1|          0|\n",
      "|      0|       13| 780|   0|          1|\n",
      "|      0|       14| 840|   1|          0|\n",
      "|      0|       15| 900|   0|          1|\n",
      "|      0|       16| 960|   0|          0|\n",
      "|      0|       17|1020|   0|          0|\n",
      "|      0|       18|1080|   0|          0|\n",
      "|      0|       19|1140|   1|          0|\n",
      "|      0|       20|1200|   1|          1|\n",
      "|      0|       21|1260|   1|          1|\n",
      "|      0|       22|1320|   1|          1|\n",
      "|      1|        0|  23|   0|       NULL|\n",
      "|      1|        1|  83|   0|          0|\n",
      "|      1|        2| 143|   1|          0|\n",
      "|      1|        3| 203|   1|          1|\n",
      "|      1|        4| 263|   1|          1|\n",
      "|      1|        5| 323|   0|          1|\n",
      "|      1|        6| 383|   0|          0|\n",
      "+-------+---------+----+----+-----------+\n",
      "only showing top 30 rows\n",
      "\n",
      "<class 'list'>\n",
      "+-------+---------+----+----+-----------+----------------+\n",
      "|BlockID|window_id|when|S117|lagged_S117|haSwitchato_S117|\n",
      "+-------+---------+----+----+-----------+----------------+\n",
      "|      0|        0|   0|   0|       NULL|               0|\n",
      "|      0|        1|  60|   0|          0|               0|\n",
      "|      0|        2| 120|   1|          0|               1|\n",
      "|      0|        3| 180|   1|          1|               0|\n",
      "|      0|        4| 240|   1|          1|               0|\n",
      "|      0|        5| 300|   0|          1|               1|\n",
      "|      0|        6| 360|   0|          0|               0|\n",
      "|      0|        7| 420|   0|          0|               0|\n",
      "|      0|        8| 480|   0|          0|               0|\n",
      "|      0|        9| 540|   1|          0|               1|\n",
      "|      0|       10| 600|   1|          1|               0|\n",
      "|      0|       11| 660|   0|          1|               1|\n",
      "|      0|       12| 720|   1|          0|               1|\n",
      "|      0|       13| 780|   0|          1|               1|\n",
      "|      0|       14| 840|   1|          0|               1|\n",
      "|      0|       15| 900|   0|          1|               1|\n",
      "|      0|       16| 960|   0|          0|               0|\n",
      "|      0|       17|1020|   0|          0|               0|\n",
      "|      0|       18|1080|   0|          0|               0|\n",
      "|      0|       19|1140|   1|          0|               1|\n",
      "|      0|       20|1200|   1|          1|               0|\n",
      "|      0|       21|1260|   1|          1|               0|\n",
      "|      0|       22|1320|   1|          1|               0|\n",
      "|      1|        0|  23|   0|       NULL|               0|\n",
      "|      1|        1|  83|   0|          0|               0|\n",
      "|      1|        2| 143|   1|          0|               1|\n",
      "|      1|        3| 203|   1|          1|               0|\n",
      "|      1|        4| 263|   1|          1|               0|\n",
      "|      1|        5| 323|   0|          1|               1|\n",
      "|      1|        6| 383|   0|          0|               0|\n",
      "+-------+---------+----+----+-----------+----------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sensors = [\"S117\"]\n",
    "window         = Window.partitionBy(\"BlockID\").orderBy(\"window_id\")\n",
    "lagged_columns = [lag(col(s)).over(window) for s in sensors] \n",
    "lag_names      = [f\"lagged_{s}\" for s in sensors]\n",
    "\n",
    "df_interest = dummy_df.select(\"BlockID\",\"window_id\",\"when\",*sensors)\n",
    "df_lagged = df_interest.withColumns(dict(zip(lag_names, lagged_columns)))\n",
    "df_lagged.show(30)\n",
    "\n",
    "print( type(lagged_columns) )\n",
    "switch_columns = [ when(  (col(s) != col(f\"lagged_{s}\")) ,1 ).otherwise(0) for s in sensors] \n",
    "switch_names      = [f\"haSwitchato_{s}\" for s in sensors]\n",
    "\n",
    "df_lag_switch = df_lagged.withColumns(dict(zip( switch_names, switch_columns)))\n",
    "df_lag_switch.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c22dd-db47-450d-bb84-849c2358b0ab",
   "metadata": {},
   "source": [
    "# Bool_or oppure spark_max / spark_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79250288-c10d-4e14-9306-874154f38e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group| flag|\n",
      "+-----+-----+\n",
      "|    A| true|\n",
      "|    A|false|\n",
      "|    A|false|\n",
      "|    B|false|\n",
      "|    B|false|\n",
      "|    B| true|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+--------+\n",
      "|group|any_true|\n",
      "+-----+--------+\n",
      "|    A|    true|\n",
      "|    B|    true|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"A\", True),\n",
    "    (\"A\", False),\n",
    "    (\"A\", False),\n",
    "    (\"B\", False),\n",
    "    (\"B\", False),\n",
    "    (\"B\", True)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"group\", \"flag\"])\n",
    "\n",
    "# bool_or: almeno un True per gruppo\n",
    "df_bool_or = df.groupBy(\"group\").agg(bool_or(\"flag\").alias(\"any_true\"))\n",
    "df.show()\n",
    "df_bool_or.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25bb296d-bbb0-4b8e-9f9d-9d4328131bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|value|\n",
      "+-----+-----+\n",
      "|    A|   10|\n",
      "|    A|   20|\n",
      "|    A|   30|\n",
      "|    B|   40|\n",
      "|    B|   50|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+---------+\n",
      "|group|avg_value|\n",
      "+-----+---------+\n",
      "|    B|       50|\n",
      "|    A|       30|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"A\", 10),\n",
    "    (\"A\", 20),\n",
    "    (\"A\", 30),\n",
    "    (\"B\", 40),\n",
    "    (\"B\", 50)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"group\", \"value\"])\n",
    "\n",
    "# Calcoliamo la media per ogni gruppo\n",
    "df_avg = df.groupBy(\"group\").agg(spark_max(\"value\").alias(\"avg_value\"))\n",
    "\n",
    "df.show()\n",
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe087f9-4c76-4178-83ae-02c70215a970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d8767-9055-443e-8417-3617afb772da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bfcaf-e342-4f48-9d7b-f4758ca468ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae687f09-1685-4ade-831b-6928d0bb22c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f399b24b-316f-4e1c-b959-15d2a696e43e",
   "metadata": {},
   "source": [
    "# Close Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95b77896-4b60-4933-88f8-f7a5b258ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174f47f-2f4e-464f-b914-6c7a4fbc154a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
