{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e55622-6c1e-4ce7-9b6a-66e7c05b9b36",
   "metadata": {},
   "source": [
    "# *** Choose your configuration: 0 docker, 1 cloudveneto ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851da545-cf4b-47fd-b5ad-58b94f4cb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 Docker, 1 CloudVeneto\n",
    "FIGHTER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732a165-f0e4-4a4d-bda7-8d878cccce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la porta dove vedere i jobs --- METTERE LA PROPRIA QUI ---\n",
    "SparkUI = 4043\n",
    "\n",
    "# nicolò 4040\n",
    "# marco  4041\n",
    "# francesco 4042\n",
    "# raffaele  4043\n",
    "\n",
    "# MASTER 8080 E' PER TUTTI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb13812-cf42-4545-975f-62e4e50efbe4",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%\">Porta 8080, voce Running Applications: puoi vedere se qualcuno ci sta lavorando.\n",
    "<br>\n",
    "Porta 8080, voce Workers: vedresti core e memoria usata se c'è qualcun'altro.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fe857-f085-4f00-b986-b7e78e2dc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"user_b\" # \"user_a\"  o  \"user_b\" (B SE SI E' I SECONDI A LAVORARCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668376d9-f121-415a-a9ed-91f504dbf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = 8 # 8 di 16, lasciare così se si lavora in due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db6b01-e376-4361-9139-84de9663a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Npartition = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da69cf-69b3-46be-95d3-5ee1682d1283",
   "metadata": {},
   "source": [
    "# *** Remember to close Spark Session ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5c59d-1172-405a-bafe-4754d41f8414",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233dd9bb-da1c-477d-898e-045d7c8589d3",
   "metadata": {},
   "source": [
    "sudo pkill -u $(whoami) -f \"jupyter-notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220edd20-24f9-44e7-9364-0ad0552a105f",
   "metadata": {},
   "source": [
    "<hr style=\"height:4px; background-color:black; border:none;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d92118-3f45-4d38-a2a9-730f57634aac",
   "metadata": {},
   "source": [
    "# Creation of the Spark Session and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a28d44-c369-4659-a27f-36556398bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PySpark core\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, expr, when, count, sum as spark_sum, abs as abs_,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, row_number, desc, asc,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b76336-8f50-45ca-8605-531a1f1e9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIGHTER==0:\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .appName(\"ProjectDocker\") \\\n",
    "        .config(\"spark.executor.memory\", \"1000m\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "        # SE NON FUNZIONA TOGLI I DUE CONFIG DI ARROW\n",
    "\n",
    "        # .config(\"spark.executor.memory\", \"1500m\")\n",
    "        # .config(\"spark.executor.cores\", \"1\")\\\n",
    "        # .config(\"spark.executor.instances\", \"12\")\\\n",
    "        # .config(\"spark.cores.max\", \"12\")\\\n",
    "        # .config(\"spark.default.parallelism\", \"24\")\\\n",
    "        # .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "\n",
    "elif FIGHTER==1:\n",
    "\n",
    "        os.environ[\"PYSPARK_PYTHON\"] = \"/opt/miniconda3/bin/python\"\n",
    "        os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/miniconda3/bin/python\"\n",
    "        \n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ProjectCloudVeneto\") \\\n",
    "            .master(\"spark://10.67.22.135:7077\") \\\n",
    "            .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "            .config(\"spark.scheduler.pool\", user) \\\n",
    "            .config(\"spark.scheduler.allocation.file\", \"file:///usr/local/spark/conf/fairscheduler.xml\") \\\n",
    "            .config(\"spark.cores.max\", core) \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "            .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "            .config(\"spark.ui.port\", SparkUI) \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "else : print(\"Better choose an available fighter, you little bastard.\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5025883-f632-4dd6-a155-cc400748ff88",
   "metadata": {},
   "source": [
    "# Dataset upload and partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9008e-6664-47e2-b7e2-6642464cf09e",
   "metadata": {},
   "source": [
    "### General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f13136-99cd-4587-b41a-939782920223",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIGHTER==0:\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/ProvePreliminari/SW-106.csv\")\n",
    "\n",
    "elif FIGHTER==1:\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Better choose an available fighter, you little bastard\")\n",
    "\n",
    "df = df.repartition(Npartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81051779-dc36-4e67-980b-46843f3826aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0acae8-0964-47f7-938d-762a11b32d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6ebcb-2a1e-4c9d-9f15-2600f04883f4",
   "metadata": {},
   "source": [
    "### Focus on one hardware at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9d3a5-9805-4732-868c-b40007b6dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert milliseconds into seconds\n",
    "df = df.withColumn(\"when\", spark_round(col(\"when\") / 1000).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274cc0f-fa59-4034-8939-d44866d42681",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwid_list = df.select(\"hwid\").distinct()\n",
    "hwid_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d73c3d-8f98-440b-84d8-3c650cd8db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE HERE HARDWARE TO ANALYZE\n",
    "hardware = \"SW-106\"\n",
    "df_hw = df  .filter(col(\"hwid\") == hardware)\\\n",
    "            .groupBy(\"when\")\\\n",
    "            .pivot(\"metric\")\\\n",
    "            .agg(first(\"value\"))\\\n",
    "            .withColumn(\"time\", from_unixtime(col(\"when\")))\\\n",
    "            .orderBy(\"when\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0ae50-018f-4319-8f98-1902941961ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentarily persist this dataframe, then we'll unpersist\n",
    "df_hw = df_hw.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74fc95-61c9-4880-b4ec-324cc1794705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we trigger persist transformation\n",
    "df_hw.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99b20f-f39e-496f-b541-1f3ab4a37092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b071f9-4c55-448f-87ed-ea874e8e764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [\"when\", \"time\", \"S117\", \"S118\", \"S169\", \"S170\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fa421-6c3d-4d9e-a18e-44a3f08c7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 5 rows\n",
    "df_hw.select(*selected_cols).orderBy(col(\"time\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63cd73-76c6-4746-a301-24bf44285ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show last 5 rows\n",
    "df_hw.select(*selected_cols).orderBy(col(\"time\").desc()).limit(5).orderBy(col(\"time\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c3426-d368-4a67-8b7f-6088fce72d7b",
   "metadata": {},
   "source": [
    "Usefull function to inspect a general dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ef893-9a57-4d41-8e88-f9afa940ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(df: DataFrame, sensors: list, start: int, end: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter a DataFrame by time range and select specified sensor columns.\n",
    "\n",
    "    Args:\n",
    "        df:        Input Spark DataFrame with 'when' and 'time' columns.\n",
    "        sensors:   List of sensor column names to include (e.g., ['S117', 'S118']).\n",
    "        start:  Start of the time range (Unix timestamp, in seconds).\n",
    "        end:    End of the time range (Unix timestamp, in seconds).\n",
    "\n",
    "    Returns:\n",
    "        Filtered Spark DataFrame with columns ['time', 'when', ...sensors].\n",
    "    \"\"\"\n",
    "    selected_columns = [\"when\",\"time\"] + sensors\n",
    "\n",
    "    return (\n",
    "        df.select(*selected_columns)\n",
    "          .filter((col(\"when\") >= start) & (col(\"when\") <= end))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e95f0b-c822-4459-9734-fab239bd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(df_hw, sensors=[\"S117\"], start=1601526622, end=1601531000).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f446543-3012-4f64-8855-04f3a94b9725",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Timestamp analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0f6fc-7d52-4f8b-9dab-a31bc4041a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_differences(df, when_col=\"when\", max_collect=1_000_000):\n",
    "    \"\"\"\n",
    "    Computes time differences (Δwhen) between consecutive rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame with a time column (e.g., 'when').\n",
    "        when_col (str): Name of the time column to compute differences on.\n",
    "        max_collect (int): Threshold to safely collect diffs to driver.\n",
    "\n",
    "    Returns:\n",
    "        rdd_diff (RDD): RDD of differences (current - previous).\n",
    "    \"\"\"\n",
    "    rdd_times = df.select(when_col).rdd.map(lambda row: row[when_col])\n",
    "    rdd_shifted = rdd_times.zipWithIndex().map(lambda x: (x[1], x[0]))  # (index, time)\n",
    "    rdd_prev = rdd_shifted.map(lambda x: (x[0] + 1, x[1]))              # shift by +1 index\n",
    "    rdd_joined = rdd_shifted.join(rdd_prev).sortByKey()\n",
    "    rdd_diff = rdd_joined.map(lambda x: x[1][0] - x[1][1])\n",
    "\n",
    "    num_diffs = rdd_diff.count()\n",
    "    print(f\"Number of time differences: {num_diffs}\")\n",
    "\n",
    "    if num_diffs < max_collect:\n",
    "        return rdd_diff\n",
    "    else:\n",
    "        print(\"Too many differences to collect safely.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3298f-a690-4ad9-87ce-84babcaf15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff_summary(rdd_diff, spark, top_n=5):\n",
    "    \"\"\"\n",
    "    Summarizes the RDD of time differences into a frequency table and prints top/bottom values.\n",
    "\n",
    "    Args:\n",
    "        rdd_diff (RDD): RDD of integer time differences.\n",
    "        spark (SparkSession): Active Spark session.\n",
    "        top_n (int): Number of rows to show from top and bottom.\n",
    "\n",
    "    Returns:\n",
    "        df_freq (DataFrame): DataFrame with columns ['diff', 'count'].\n",
    "    \"\"\"\n",
    "    if rdd_diff is None:\n",
    "        print(\"No differences available to summarize.\")\n",
    "        return None\n",
    "\n",
    "    df_freq = rdd_diff.map(lambda d: (d, 1)) \\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .toDF([\"diff\", \"count\"]) \\\n",
    "                      .orderBy(\"diff\")\n",
    "\n",
    "    print(f\"\\n{top_n} smallest time differences:\")\n",
    "    df_freq.show(top_n, truncate=False)\n",
    "\n",
    "    print(f\"\\n{top_n} largest time differences:\")\n",
    "    df_freq.orderBy(\"diff\", ascending=False).show(top_n, truncate=False)\n",
    "\n",
    "    return df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23163b-0c73-4674-8401-5024b4276d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time differences\n",
    "rdd_diff = compute_time_differences(df_hw)\n",
    "\n",
    "# Summarize and print top/bottom time gaps\n",
    "df_diff_summary = time_diff_summary(rdd_diff, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988ee9a-11ad-4296-bb6b-4af7dfda97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_histogram(differences, min_diff, max_diff, delta):\n",
    "    \n",
    "    filtered = [d for d in differences if min_diff <= d <= max_diff]\n",
    "\n",
    "    bins = int((max_diff - min_diff) / delta)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered, bins=bins, color=\"steelblue\", edgecolor=\"black\")\n",
    "    plt.title(f\"Time Difference Events (range {min_diff} - {max_diff} [s])\")\n",
    "    plt.xlabel(\"Time difference [s]\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    #plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52466-b638-4f78-a69b-675637894c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = rdd_diff.collect() # check previously the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fc7ad-7b64-43ba-a24a-c792ebcbba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diff_histogram(differences=diffs, min_diff=0, max_diff=180, delta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddab6d-ec14-4fa8-852b-1b82c8e37477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diff_histogram(differences=diffs, min_diff=180, max_diff=4000, delta=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a42ed-503b-48f1-8424-3797e172ef8a",
   "metadata": {},
   "source": [
    "# Handling missing data & gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf468733-9925-4c0d-9103-0fe3bb49a50c",
   "metadata": {},
   "source": [
    "### Create final grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbacf3-8b46-4a7e-8c50-d58ef6cffb72",
   "metadata": {},
   "source": [
    "Direi di tenere questa funzione \"FillGaps\" e continuare ad implementare partendo da questa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5fead-148a-4ea9-ac9d-99c24593657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 60 #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e45e5-4245-4719-b11f-30dec4d4afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FillGaps(\n",
    "    df: DataFrame,\n",
    "    sensors: list = None,\n",
    "    interval: int = 60,\n",
    "    modality: str = 'auto',\n",
    "    fill_null: bool = False\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates sensor data into fixed-size time windows and fills missing values using Spark.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame with a 'when' column (UNIX timestamp in seconds).\n",
    "        sensors: List of sensor column names. If None, inferred from all columns except 'when' and 'time'.\n",
    "        interval: Window size in seconds.\n",
    "        modality: Aggregation method: 'mean', 'min', 'max', 'mode', or 'auto'.\n",
    "        fill_null: If True, fills missing values using forward and backward fill in Spark.\n",
    "\n",
    "    Returns:\n",
    "        Aggregated and optionally gap-filled DataFrame, with a `when` column at the center of the window\n",
    "        and a `window_id` column that uniquely identifies each time window.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Infer sensor columns if not provided\n",
    "    if sensors is None:\n",
    "        sensors = [c for c in df.columns if c not in (\"when\", \"time\")]\n",
    "\n",
    "    # 2. Add timestamp column\n",
    "    df_ts = df.withColumn(\"timestamp\", from_unixtime(col(\"when\")).cast(\"timestamp\"))\n",
    "\n",
    "    # 3. Create time window\n",
    "    df_windowed = df_ts.withColumn(\"time_window\", window(\"timestamp\", f\"{interval} seconds\"))\n",
    "\n",
    "    # 4. Aggregate using selected modality\n",
    "    if modality == \"mode\":\n",
    "        # Special case: MODE needs groupBy and count per window + sensor\n",
    "        aggs = []\n",
    "        for s in sensors:\n",
    "            mode_df = (\n",
    "                df_windowed.groupBy(\"time_window\", s)\n",
    "                .agg(count(\"*\").alias(\"cnt\"))\n",
    "                .withColumn(\"rank\", row_number().over(\n",
    "                    Window.partitionBy(\"time_window\").orderBy(desc(\"cnt\"))\n",
    "                ))\n",
    "                .filter(col(\"rank\") == 1)\n",
    "                .select(\"time_window\", col(s).alias(s))\n",
    "            )\n",
    "            if not aggs:\n",
    "                result_df = mode_df\n",
    "            else:\n",
    "                result_df = result_df.join(mode_df, on=\"time_window\", how=\"outer\")\n",
    "    else:\n",
    "        aggs = []\n",
    "        for s in sensors:\n",
    "            if modality == \"mean\":\n",
    "                agg_func = spark_avg(col(s)).alias(s)\n",
    "            elif modality == \"min\":\n",
    "                agg_func = spark_min(col(s)).alias(s)\n",
    "            elif modality == \"max\":\n",
    "                agg_func = spark_max(col(s)).alias(s)\n",
    "            elif modality == \"auto\":\n",
    "                stats = df.selectExpr(f\"min({s}) as min\", f\"max({s}) as max\").first()\n",
    "                is_binary = stats[\"min\"] is not None and stats[\"max\"] is not None and 0 <= stats[\"min\"] and stats[\"max\"] <= 1\n",
    "                if s in [\"A5\", \"A9\"] or is_binary:\n",
    "                    agg_func = spark_max(col(s)).alias(s)\n",
    "                else:\n",
    "                    agg_func = spark_avg(col(s)).alias(s)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported modality: {modality}\")\n",
    "            aggs.append(agg_func)\n",
    "\n",
    "        result_df = (\n",
    "            df_windowed\n",
    "            .groupBy(\"time_window\")\n",
    "            .agg(*aggs)\n",
    "        )\n",
    "\n",
    "    # 5. Add window_start, window_end, and 'when' as center of window\n",
    "    result_df = (\n",
    "        result_df\n",
    "        .withColumn(\"window_start\", col(\"time_window.start\"))\n",
    "        .withColumn(\"window_end\", col(\"time_window.end\"))\n",
    "        .withColumn(\"when\", expr(\"unix_timestamp(window_start) + int((unix_timestamp(window_end) - unix_timestamp(window_start)) / 2)\"))\n",
    "        .drop(\"time_window\")\n",
    "        .orderBy(\"when\")\n",
    "    )\n",
    "\n",
    "    # 6. Fill nulls if requested using Spark-native ffill + bfill\n",
    "    if fill_null:\n",
    "        w_forward = Window.orderBy(\"when\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "        w_backward = Window.orderBy(\"when\").rowsBetween(0, Window.unboundedFollowing)\n",
    "        for s in sensors:\n",
    "            result_df = result_df.withColumn(s, last(col(s), ignorenulls=True).over(w_forward))\n",
    "            result_df = result_df.withColumn(s, first(col(s), ignorenulls=True).over(w_backward))\n",
    "\n",
    "    # 7. Add window_id as progressive row number\n",
    "    result_df = result_df.withColumn(\"window_id\", monotonically_increasing_id())\n",
    "\n",
    "    return result_df.select([\"window_id\", \"when\", \"window_start\", \"window_end\"] + sensors)\n",
    "\n",
    "    # 6. Optionally fill nulls using Pandas (costly)\n",
    "    # if fill_null:\n",
    "    #     pandas_df = result_df.toPandas()\n",
    "    #     pandas_df = pandas_df.ffill().bfill()\n",
    "    #     return spark.createDataFrame(pandas_df)\n",
    "    # else:\n",
    "    #     return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d6a5b-bea0-44f5-9467-cc3ee04c8eb0",
   "metadata": {},
   "source": [
    "Already added conversion of A5 and A9 sensors, and overheating control function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb321d1-2adb-49b0-8964-5954fc64d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alarm_bits(df: DataFrame, columns=[\"A5\", \"A9\"], bits=[6, 7, 8]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts specific bits (1-indexed, left to right) from the given integer alarm columns\n",
    "    and adds them as new columns in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input Spark DataFrame.\n",
    "        columns: List of alarm column names (e.g., [\"A5\", \"A9\"]).\n",
    "        bits: List of bit positions to extract (left-to-right, 1-based index).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with additional columns like \"6-A5\", \"7-A9\", etc.\n",
    "    \"\"\"\n",
    "    for col_name in columns:\n",
    "        for bit in bits:\n",
    "            # Convert left-to-right to right-to-left (bit 1 is MSB → position 15)\n",
    "            bit_from_right = 16 - bit\n",
    "            df = df.withColumn(\n",
    "                f\"{bit}-{col_name}\",\n",
    "                ((col(col_name).bitwiseAND(1 << bit_from_right)) > 0).cast(\"int\")\n",
    "            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bba7e7-48ab-48fa-8c62-38f68920310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_overheating_flag(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a binary column 'overheating' which is 1 if any of bits 6, 7, or 8 of A5 or A9 is 1.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame with bit columns already extracted.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 'overheating' column added.\n",
    "    \"\"\"\n",
    "    overheating_bits = [f\"{b}-{s}\" for s in [\"A5\", \"A9\"] for b in [6, 7, 8]]\n",
    "    condition = sum([col(c) for c in overheating_bits]) > 0\n",
    "    return df.withColumn(\"overheating\", when(condition, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad2a85-6f6d-4af8-abff-0891fc6dae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #2° MODALITA' USO FUNZIONE FillGaps\n",
    "# #Doesn't seem to be particularly dependent on the number of metrics (9 seconds with 1, 13 seconds with 15)\n",
    "# sensors=['S117', 'S118',  'S169', 'S170', 'S41', 'ComError']#, 'A5', 'A9', 'P18', 'P2', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1']\n",
    "# GridDF = FillGaps(df_hw, sensors=sensors, modality=\"min\", interval=60)\n",
    "# GridDF.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b10bd4-2fae-45c1-9fab-389d96d8bd94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#1° MODALITA' USO FUNZIONE FillGaps\n",
    "# Fill sensor gaps\n",
    "df_grid = FillGaps(df_hw, interval=frequency, modality=\"auto\", fill_null=False)\n",
    "\n",
    "# Extract bits from alarms\n",
    "df_grid = extract_alarm_bits(df_grid, columns=[\"A5\", \"A9\"], bits=[6, 7, 8])\n",
    "\n",
    "# Add 'overheating' column\n",
    "# --- IMPLEMENTATA GIA (E' RICHIESTA PIU' AVANTI) ORA NON SERVE\n",
    "# df_final = add_overheating_flag(df_final)\n",
    "\n",
    "# Now persist the result\n",
    "df_final = df_grid.persist()\n",
    "\n",
    "# Trigger persist\n",
    "df_final.show(1, truncate = False)\n",
    "\n",
    "# Unpersist original dataframe\n",
    "# --- PER IL MOMENTO DIREI DI TENERLO CON IL PERSIST CHE E' PRATICO PER LA FASE DI SVILUPPO PER RICONTROLLARE LE COSE SUL DATAFRAME ORIGINARIO\n",
    "# df_hw.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32363f5b-f834-41c1-a348-a8b31d4ed446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.select(\"window_id\",\"when\",\"window_start\",\"window_end\",\"S110\",\"S112\",\"S113\",\"S114\",\"S115\",\"S117\",\"S118\",\"S122\",\"A9\",\"8-A9\").show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833128e3-1b4a-4d52-832b-1496e9e726d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect(df_hw, sensors=[\"S110\",\"S112\",\"S113\",\"S114\",\"S115\",\"S117\",\"S118\",\"S122\",\"A9\"], start=1601510400, end=1601512550).show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82ff09-e754-431e-89d5-8e50e13dd429",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Block distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20762553-d385-4698-a1d6-1ce2c8bcc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_blocks(df_final: DataFrame, frequency: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Build blocks using existing window_start for partitioning.\n",
    "    \n",
    "    Args:\n",
    "        df_final: Input Spark DataFrame with 'when' and 'window_start' columns.\n",
    "        frequency: Maximum allowed time gap in seconds.\n",
    "    Returns:\n",
    "        DataFrame with columns: 'window_id', 'block_id'.\n",
    "    \"\"\"\n",
    "    # 1) Extract date from existing window_start column for partitioning\n",
    "    df_with_day = df_final.withColumn(\"day\", to_date(col(\"window_start\")))\n",
    "    \n",
    "    # 2) Window partitioned by day, ordered by when\n",
    "    window_spec = Window.partitionBy(\"day\").orderBy(\"when\")\n",
    "    \n",
    "    # 3) Compute gap to previous timestamp within each day\n",
    "    df_with_gap = (df_with_day\n",
    "        .withColumn(\"prev_when\", lag(\"when\", 1).over(window_spec))\n",
    "        .withColumn(\"gap\", col(\"when\") - col(\"prev_when\"))\n",
    "    )\n",
    "    \n",
    "    # 4) Flag start of new block\n",
    "    df_with_flag = df_with_gap.withColumn(\n",
    "        \"is_new_block\",\n",
    "        when((col(\"prev_when\").isNull()) | (col(\"gap\") > frequency), 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # 5) Create daily block_id with partitioned window\n",
    "    df_with_daily_blocks = df_with_flag.withColumn(\n",
    "        \"daily_block_id\",\n",
    "        spark_sum(\"is_new_block\").over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # 6) Create global block_id - get max blocks per day\n",
    "    daily_max = (df_with_daily_blocks\n",
    "        .groupBy(\"day\")\n",
    "        .agg(spark_max(\"daily_block_id\").alias(\"max_block\"))\n",
    "    )\n",
    "    \n",
    "    # 7) Calculate day offsets with partitioned window\n",
    "    day_window = Window.orderBy(\"day\")\n",
    "    daily_offsets = (daily_max\n",
    "        .withColumn(\"prev_max\", lag(\"max_block\", 1).over(day_window))\n",
    "        .withColumn(\"day_offset\", \n",
    "            spark_sum(when(col(\"prev_max\").isNull(), 0).otherwise(col(\"prev_max\"))).over(day_window))\n",
    "        .fillna(0, subset=[\"day_offset\"])\n",
    "        .select(\"day\", \"day_offset\")\n",
    "    )\n",
    "    \n",
    "    # 8) Join and create final block_id\n",
    "    result = (df_with_daily_blocks\n",
    "        .join(daily_offsets, \"day\")\n",
    "        .withColumn(\"block_id\", col(\"daily_block_id\") + col(\"day_offset\") - 1)\n",
    "        .select(\"window_id\", \"block_id\")\n",
    "        .orderBy(\"when\")  # Final sort by time\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b5966-b043-4d79-97f2-ac0836724fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks = build_blocks(df_final, frequency=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fb073-a5df-43f1-9332-50fe31cf9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea97fd-7bde-403b-8020-ab78f691a423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.join(df_blocks, on=\"window_id\", how=\"left\").select(\"block_id\",\"window_id\",\"window_start\").orderBy(\"window_id\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539662d-3672-4ef4-9422-cb1644db9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block_duration = (\n",
    "    df_blocks.groupBy(\"block_id\")\n",
    "    .count()\n",
    "    .withColumn(\"duration_minutes\", (col(\"count\") * lit(frequency)) / 60)\n",
    "    .drop(\"count\")\n",
    "    .orderBy(\"block_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ad1ed-b278-4a11-8b77-dc0d922946ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block_duration.show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23771bf-a689-49c4-bc51-dccf3858a2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_block_stats = df_block_duration.groupBy(col(\"duration_minutes\").alias(\"block_length\")) \\\n",
    "    .agg(spark_count(\"*\").alias(\"block_count\")).orderBy(\"block_length\")\n",
    "df_block_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22e7fa-6af6-4cfd-87e5-f7a7b621144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_histogram(df_block_stats, min_length=1, max_length=100, step=1):\n",
    "    \"\"\"\n",
    "    Plot a histogram of contiguous block lengths.\n",
    "\n",
    "    Args:\n",
    "      df_block_stats: Spark DataFrame with ['block_length','block_count']\n",
    "      min_length:     smallest block_length to include\n",
    "      max_length:     largest block_length to include\n",
    "      step:           bar width\n",
    "    \"\"\"\n",
    "    pdf = (\n",
    "        df_block_stats\n",
    "        .filter((col(\"block_length\") >= min_length) &\n",
    "                (col(\"block_length\") <= max_length))\n",
    "        .orderBy(\"block_length\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    if pdf.empty:\n",
    "        print(\"No blocks in the specified range.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(pdf[\"block_length\"], pdf[\"block_count\"], width=step, align=\"center\")\n",
    "    plt.xlabel(\"Block Length\")\n",
    "    plt.ylabel(\"Number of Blocks\")\n",
    "    plt.title(\"Histogram of Contiguous Block Lengths\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(min_length, max_length)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44872ac-7a65-499b-b0dd-48ccbd38a96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_diff_histogram(df_block_stats, min_length=1, max_length=100, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485cb6e-6c4d-42cd-9232-6c14abaa16e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43359f-a6ef-4ae2-a638-d87ad821b9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bfa5d-3007-404b-8a08-0c70ce915c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "299f715d-63c1-4925-8f41-7bfc13ec3abe",
   "metadata": {},
   "source": [
    "### Anomaly detection (Raffaele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9a30b-b7f2-49a2-9928-511fb064bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def resample_sensors(df, sensors, time_int):\n",
    "    \n",
    "    # Crea dataframe con sensori ricampionati. \n",
    "    \n",
    "    # Trovo istante iniziale\n",
    "    min_time = df.agg(spark_min(unix_timestamp('time')).alias('min_time')).collect()[0]['min_time']\n",
    "\n",
    "    # Creo lista di aggregatori da usare per mediare i dati vicini di ogni colonna\n",
    "    aggs = [mode(col(s)).alias(f\"mode_{s}\") for s in sensors] # mode prende il valore più frequente nel gruppo\n",
    "\n",
    "    # Creo df dove ogni dato della stessa colonna in [t, t+time_int] è mediato\n",
    "    resampled_df = df \\\n",
    "        .select('time', *sensors) \\\n",
    "        .withColumn('window_idx', spark_round((unix_timestamp(col('time'))-min_time)/time_int)) \\\n",
    "        .groupBy('window_idx').agg(*aggs) \\\n",
    "        .withColumn('sampling_time', from_unixtime(col('window_idx')*time_int + min_time)) \\\n",
    "        .withColumn('when', unix_timestamp(col('sampling_time'))) \\\n",
    "        .orderBy('sampling_time')\n",
    "    return resampled_df\n",
    "'''\n",
    "\n",
    "############################################################################\n",
    "\n",
    "\n",
    "def detect_switch_anomalies(df, sensors, window_minutes=10, switch_threshold=5):\n",
    "    \"\"\"\n",
    "    Crea un dataframe che contiene una flag per le anomalie di ogni sensore ogni window_minutes \n",
    "    NB: siccome la grid è spaziata di 1 minuto, window_minutes deve essere > 1 per avere senso\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract day (or hour) from timestamp to partition\n",
    "    df = df \\\n",
    "        .select('when', *sensors, 'window_start', 'window_end') \\\n",
    "        .withColumn(\"day\", to_date(col('window_start')))\n",
    "\n",
    "    # Lag to get previous value within each partition --- OTHERWISE IT MOVES ALL IN THE SAME PARTITION \n",
    "    w_lag = Window.partitionBy(\"day\").orderBy(\"when\")\n",
    "    lagged_columns = [lag(col(s)).over(w_lag) for s in sensors] # per ogni sensore prendo le righe shiftate di 1 all'indietro\n",
    "    lag_names      = [f\"lagged_{s}\" for s in sensors]\n",
    "    \n",
    "    df_lagged = df.withColumns(dict(zip(lag_names, lagged_columns)))\n",
    "\n",
    "    # Detect change 0→1 or 1→0\n",
    "    didSwitch   = [when((col(f\"lagged_{s}\") != col(s)), 1).otherwise(0) for s in sensors] # per ogni sensore verifico se il valore è cambiato rispetto al precedente \n",
    "    switch_names = [f\"switch_{s}\" for s in sensors]\n",
    "    \n",
    "    df_changes = df_lagged.withColumns(dict(zip(switch_names, didSwitch)))\n",
    "\n",
    "    # Rolling window over time with partition\n",
    "    # Raffaele: così rischiamo di contare più volte la stessa anomalia.\n",
    "     '''\n",
    "    w_time = Window.partitionBy(\"day\").orderBy(\"when\").rangeBetween(-window_minutes * 60, 0)\n",
    "    change_counts = [spark_sum(f\"switch_{s}\").over(w_time) for s in sensors]\n",
    "    counts_names  = [f\"change_count_{s}\" for s in sensors]\n",
    "    \n",
    "    df_windowed = df_changes.withColumns(dict(zip(counts_names, change_counts)))\n",
    "     '''\n",
    "    w_group = Window.orderBy('when')\n",
    "    w_count = Window.partitionBy('groupId')\n",
    "    \n",
    "    df_switch_group = (\n",
    "        df_changes \\\n",
    "        .withColumn('switch_group', when(lag(col('switch_S117')) != col('switch_S117'), 1).otherwise(0)) \\\n",
    "        .withColumn('groupId', spark_sum('switch_group').over(w_group)) \\\n",
    "        .withColumn('')\n",
    "    )\n",
    "\n",
    "    # Flag anomaly\n",
    "    anomalies       = [col(f\"change_count_{s}\") > switch_threshold for s in sensors]\n",
    "    anomalies_names = [f\"{s}_anomaly\" for s in sensors]\n",
    "    \n",
    "    df_anomaly = df_windowed \\\n",
    "    .withColumns(dict(zip(anomalies_names, anomalies))) \\\n",
    "    .drop(*lag_names, *switch_names, *counts_names, \"day\", \"window_idx\")\n",
    "\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49728a35-64fc-4b0e-a6ad-08cd647d26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = ['S117', 'S118', 'S169', 'S170']\n",
    "anomalies = detect_switch_anomalies(\n",
    "  df= df_final,\n",
    "  sensors=sensors,\n",
    "  window_minutes=60,\n",
    "  switch_threshold=6\n",
    ")\n",
    "\n",
    "# Per confronto vediamo se le anomalie del S117 corrispondono a quelle del metodo di Marco:\n",
    "anomalies \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# Questo non va ancora\n",
    "#filters = [col(f\"mode_{s}_anomaly\") for s in sensors] \n",
    "#total_filter = filters[0]\n",
    "#for i in range(1, len(filters)):\n",
    "#    total_filter = total_filter & filters[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4ae3f-7772-4400-9541-bc41f9f587d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_hist(df, sensors, time_start, time_end):\n",
    "    '''\n",
    "    Plotta il numero di anomalie al giorno per ogni sensore, nell'arco di tempo [time_start, time_end]\n",
    "    '''\n",
    "\n",
    "    casts = [col(f'{s}_anomaly').cast('Int') for s in sensors] # per convertire i booleani in S_anomaly in 0/1 \n",
    "    aggs = [spark_sum(col(f'{s}_anomaly')).alias(f'anomaly_count_{s}') for s in sensors] # per contare le anomalie ogni giorno\n",
    "\n",
    "    new_df = df \\\n",
    "    .select('when', 'window_start', 'window_end', *[f\"{s}_anomaly\" for s in sensors]) \\\n",
    "    .filter(col('when').between(time_start, time_end)) \\\n",
    "    .withColumn('day', to_date(col('window_start'))) \\\n",
    "    .withColumns(dict(zip([f'{s}_anomaly' for s in sensors], casts))) \\\n",
    "    .groupBy('day').agg(*aggs) \\\n",
    "    .orderBy('day')\n",
    "\n",
    "    n_sensors = len(sensors)\n",
    "    fig, axes = plt.subplots(ceil(n_sensors/2), 2, figsize=(20, 5))\n",
    "    days = new_df.select('day').rdd.flatMap(lambda x: x).collect()\n",
    "    for i in range(n_sensors):\n",
    "        counts = new_df.select(f'anomaly_count_{sensors[i]}').rdd.flatMap(lambda x: x).collect()\n",
    "        idx = i if n_sensors <= 2 else (i//2, i%2)\n",
    "        axes[idx].bar(days, counts)\n",
    "        axes[idx].set_title(f'anomaly_count_{sensors[i]}')\n",
    "        axes[idx].set(xlabel=\"day\", ylabel=\"counts\")\n",
    "        #print(sensors[i], counts)\n",
    "    fig.tight_layout()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e15d87-3ea8-426e-b432-5a94d04aed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomaly_hist(anomalies, ['S117', 'S118', 'S169', 'S170'], 1602879000, 1602879000+10000000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f463a-5d59-43b2-854e-f69d3d866df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f98d6-1af1-4e82-8c7e-46b25d3b9479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cc736-c6eb-48c5-ae48-605ecaacdc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e90eec7a-6138-4167-8f78-58bafab1248e",
   "metadata": {},
   "source": [
    "# <hr style=\"height:4px; background-color:black; border:none;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca02a0-e945-4d6a-9e70-237afb583c78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Su solo una colonna (metodo nearest) FUNZIONANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb44695-86d4-4d5d-8af8-93de34608f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_sensor_simple(\n",
    "    df_hard: DataFrame,\n",
    "    sensor: str,\n",
    "    interval: int = 60\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resample a single sensor column to regular intervals using the nearest non-null value.\n",
    "\n",
    "    Args:\n",
    "        df_hard:   Spark DataFrame with columns ['when', sensor].\n",
    "        sensor:    Name of the sensor column (e.g., 'S117').\n",
    "        interval:  Resample interval in seconds.\n",
    "\n",
    "    Returns:\n",
    "        Resampled DataFrame with ['when', sensor, 'time'], using only non-null nearest values.\n",
    "    \"\"\"\n",
    "    spark = df_hard.sparkSession\n",
    "    half = interval / 2\n",
    "\n",
    "    # 1. Create target timestamp grid\n",
    "    bounds = df_hard.selectExpr(\"min(when) as min_t\", \"max(when) as max_t\").first()\n",
    "    min_t, max_t = bounds.min_t, bounds.max_t\n",
    "\n",
    "    grid_df = (\n",
    "        spark.range(1)\n",
    "             .selectExpr(f\"sequence({min_t}, {max_t}, {interval}) as times\")\n",
    "             .select(explode(\"times\").alias(\"target_when\"))\n",
    "    )\n",
    "\n",
    "    # 2. Range join within ±half interval and exclude NULLs before aggregation\n",
    "    joined = (\n",
    "        df_hard\n",
    "          .select(\"when\", sensor)\n",
    "          .filter(col(sensor).isNotNull())  # <-- filter out nulls before min_by\n",
    "          .join(\n",
    "              grid_df,\n",
    "              (col(\"when\") >= col(\"target_when\") - half) &\n",
    "              (col(\"when\") <= col(\"target_when\") + half),\n",
    "              how=\"inner\"\n",
    "          )\n",
    "          .withColumn(\"time_diff\", abs_(col(\"when\") - col(\"target_when\")))\n",
    "    )\n",
    "\n",
    "    # 3. Pick value with minimum time difference (nearest non-null)\n",
    "    result = (\n",
    "        joined.groupBy(\"target_when\")\n",
    "              .agg(min_by(col(sensor), col(\"time_diff\")).alias(sensor))\n",
    "    )\n",
    "\n",
    "    # 4. Format output\n",
    "    final = (\n",
    "        result\n",
    "          .withColumnRenamed(\"target_when\", \"when\")\n",
    "          .withColumn(\"time\", from_unixtime(col(\"when\")))\n",
    "          .orderBy(\"when\")\n",
    "    )\n",
    "\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89425435-d21b-4e22-8357-4074901dc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_resampled_S117 = resample_sensor_simple(df_hard, \"A5\", interval=60)\n",
    "df_resampled_S117.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326d19b-fcfc-4259-a540-b36a948b1297",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Various checks to show that everything works (questi li terrei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520425d-e090-433c-96f1-20fdbddce8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a specific region\n",
    "start_ts = 1601526622\n",
    "end_ts   = 1601531000\n",
    "\n",
    "df_resampled_S117.filter(\n",
    "    (col(\"when\") >= start_ts) & \n",
    "    (col(\"when\") <= end_ts)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec865f-5198-4bc9-9ad7-74b2d9b0ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(df_hard, sensors=[\"S117\"], start=1601526622, end=1601531000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1bdac-8a39-451c-9c8e-52d60b9ea15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(df_hard, sensors=[\"S117\"], start=1601527100, end=1601529400).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5de846-c5a6-4ad1-8121-3df91435ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check everything is fine\n",
    "\n",
    "# Compute time differences\n",
    "rdd_diff_S117 = compute_time_differences(df_resampled_S117)\n",
    "\n",
    "# Summarize and print top/bottom time gaps\n",
    "df_diff_summary_S117 = time_diff_summary(rdd_diff_S117, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dce3a2-75a5-430a-b18b-6e5f4f35a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questo dai dati di prima, sembra funzionare\n",
    "# |5003774|1    |\n",
    "# |2066908|1    |\n",
    "# |757864 |1    |\n",
    "# |48451  |1    |\n",
    "# |28598  |1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff157f-db7d-489b-8873-cd4416380597",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### data blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2198e8-7fc1-4d5b-b7b8-eca3b743769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contiguous_blocks(\n",
    "    df: DataFrame,\n",
    "    interval: int,\n",
    "    col_when: str = \"when\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Count contiguous blocks of rows spaced exactly `interval` seconds apart,\n",
    "    partitioned by day for parallelism.\n",
    "\n",
    "    Args:\n",
    "      df:         Spark DataFrame with a timestamp column in seconds.\n",
    "      interval:   Expected spacing between consecutive rows (in seconds).\n",
    "      col_when:   Name of that timestamp column (default 'when').\n",
    "\n",
    "    Returns:\n",
    "      DataFrame with columns:\n",
    "        - block_length: number of rows in each contiguous block\n",
    "        - block_count:  how many such blocks exist\n",
    "    \"\"\"\n",
    "    # 1) Add a 'day' column for partitioning\n",
    "    df2 = df.withColumn(\"day\", to_date(from_unixtime(col(col_when))))\n",
    "\n",
    "    # 2) Window over each day, ordered by timestamp\n",
    "    w = Window.partitionBy(\"day\").orderBy(col(col_when))\n",
    "\n",
    "    # 3) Compute the gap to the previous timestamp\n",
    "    df3 = (\n",
    "        df2\n",
    "        .withColumn(\"prev_when\", lag(col(col_when), 1).over(w))\n",
    "        .withColumn(\"gap\", col(col_when) - col(\"prev_when\"))\n",
    "    )\n",
    "\n",
    "    # 4) Flag start of new block when gap != interval (or first in day)\n",
    "    df4 = df3.withColumn(\n",
    "        \"is_new_block\",\n",
    "        expr(f\"CASE WHEN gap IS NULL OR gap != {interval} THEN 1 ELSE 0 END\")\n",
    "    )\n",
    "\n",
    "    # 5) Cumulative sum over the window to assign a block_id\n",
    "    df5 = df4.withColumn(\n",
    "        \"block_id\",\n",
    "        sum_(col(\"is_new_block\")).over(w)\n",
    "    )\n",
    "\n",
    "    # 6) Count rows per (day, block_id) → this is block_length\n",
    "    blocks = (\n",
    "        df5\n",
    "        .groupBy(\"day\", \"block_id\")\n",
    "        .agg(count(\"*\").alias(\"block_length\"))\n",
    "    )\n",
    "\n",
    "    # 7) Count how many blocks have each length across all days\n",
    "    result = (\n",
    "        blocks\n",
    "        .groupBy(\"block_length\")\n",
    "        .agg(count(\"*\").alias(\"block_count\"))\n",
    "        .orderBy(\"block_length\")\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2ca2b-a4fa-4d2a-a6c8-4087f351ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_histogram(df_block_stats, min_length=1, max_length=100, step=1):\n",
    "    \"\"\"\n",
    "    Plot a histogram of contiguous block lengths.\n",
    "\n",
    "    Args:\n",
    "      df_block_stats: Spark DataFrame with ['block_length','block_count']\n",
    "      min_length:     smallest block_length to include\n",
    "      max_length:     largest block_length to include\n",
    "      step:           bar width\n",
    "    \"\"\"\n",
    "    pdf = (\n",
    "        df_block_stats\n",
    "        .filter((col(\"block_length\") >= min_length) &\n",
    "                (col(\"block_length\") <= max_length))\n",
    "        .orderBy(\"block_length\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    if pdf.empty:\n",
    "        print(\"No blocks in the specified range.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(pdf[\"block_length\"], pdf[\"block_count\"], width=step, align=\"center\")\n",
    "    plt.xlabel(\"Block Length\")\n",
    "    plt.ylabel(\"Number of Blocks\")\n",
    "    plt.title(\"Histogram of Contiguous Block Lengths\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(min_length, max_length)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef75a4e-3df8-4478-96cc-fe428203f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Select only the timestamp column\n",
    "df_sample = df_resampled_S117.select(\"when\")\n",
    "\n",
    "# 2) Compute contiguous block stats at 60s spacing\n",
    "block_stats = count_contiguous_blocks(df_sample, interval=60)\n",
    "\n",
    "# 3) Inspect\n",
    "block_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a96ce-c606-4b71-a820-587c173eda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Plot histogram for blocks length 1–1000\n",
    "plot_diff_histogram(block_stats, min_length=1, max_length=100, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373b431-09fb-4be5-b205-237db8c42fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b6528-8c7c-45c1-98b7-d64548d488d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf311c7-91c5-42c2-b69d-4ce24bee430e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Con vari metodi riempimento (near,min,max,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6920ce8-101c-4288-aba4-92ff27f528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_sensor(\n",
    "    df_hard: DataFrame,\n",
    "    sensor: str,\n",
    "    interval: int = 60,\n",
    "    method: str = \"near\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resample a single sensor column to regular intervals.\n",
    "\n",
    "    Args:\n",
    "      df_hard:   DataFrame with 'when' (in seconds) and the sensor column.\n",
    "      sensor:    name of the sensor column (e.g. 'S117').\n",
    "      interval:  interval in seconds (e.g. 60).\n",
    "      method:    one of 'near', 'min', 'max', 'mean'.\n",
    "\n",
    "    Returns:\n",
    "      DataFrame with columns ['when', sensor, 'time'] at regular timestamps.\n",
    "      All values are non-null. If no data is found for a window, the row is skipped.\n",
    "    \"\"\"\n",
    "    spark = df_hard.sparkSession\n",
    "    half = interval / 2\n",
    "\n",
    "    # Validate method\n",
    "    if method not in (\"near\", \"min\", \"max\", \"mean\"):\n",
    "        raise ValueError(f\"Unsupported method '{method}'. Choose from 'near', 'min', 'max', 'mean'.\")\n",
    "\n",
    "    # 1. Create time grid\n",
    "    bounds = df_hard.selectExpr(\"min(when) as min_t\", \"max(when) as max_t\").first()\n",
    "    min_t, max_t = bounds.min_t, bounds.max_t\n",
    "\n",
    "    grid_df = (\n",
    "        spark.range(1)\n",
    "             .selectExpr(f\"sequence({min_t}, {max_t}, {interval}) as times\")\n",
    "             .select(explode(\"times\").alias(\"target_when\"))\n",
    "    )\n",
    "\n",
    "    # 2. Join with range ± half interval and exclude NULLs before aggregation\n",
    "    filtered = df_hard.select(\"when\", sensor).filter(col(sensor).isNotNull())\n",
    "\n",
    "    joined = (\n",
    "        filtered\n",
    "          .join(\n",
    "              grid_df,\n",
    "              (col(\"when\") >= col(\"target_when\") - half) &\n",
    "              (col(\"when\") <= col(\"target_when\") + half),\n",
    "              how=\"inner\"\n",
    "          )\n",
    "          .withColumn(\"time_diff\", abs_(col(\"when\") - col(\"target_when\")))\n",
    "    )\n",
    "\n",
    "    # 3. Aggregate based on method\n",
    "    if method == \"near\":\n",
    "        agg_expr = min_by(col(sensor), col(\"time_diff\")).alias(sensor)\n",
    "    elif method == \"min\":\n",
    "        agg_expr = spark_min(col(sensor)).alias(sensor)\n",
    "    elif method == \"max\":\n",
    "        agg_expr = spark_max(col(sensor)).alias(sensor)\n",
    "    else:  # mean\n",
    "        agg_expr = spark_avg(col(sensor)).alias(sensor)\n",
    "\n",
    "    result = (\n",
    "        joined.groupBy(\"target_when\")\n",
    "              .agg(agg_expr)\n",
    "    )\n",
    "\n",
    "    # 4. Final formatting\n",
    "    final = (\n",
    "        result\n",
    "          .withColumnRenamed(\"target_when\", \"when\")\n",
    "          .withColumn(\"time\", from_unixtime(col(\"when\")))\n",
    "          .orderBy(\"when\")\n",
    "    )\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d2465-73f7-469c-a076-0b632c8148dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest‐time fill\n",
    "df_near = resample_sensor(df_hard, \"S117\", interval=60, method=\"near\")\n",
    "df_near.show(5)\n",
    "\n",
    "# Minimum‐value fill\n",
    "df_min = resample_sensor(df_hard, \"S117\", interval=60, method=\"min\")\n",
    "df_min.show(5)\n",
    "\n",
    "# Maximum‐value fill\n",
    "df_max = resample_sensor(df_hard, \"S117\", interval=60, method=\"max\")\n",
    "df_max.show(5)\n",
    "\n",
    "# Mean‐value fill\n",
    "df_mean = resample_sensor(df_hard, \"S117\", interval=60, method=\"mean\")\n",
    "df_mean.show(5)\n",
    "\n",
    "# Invalid method raises:\n",
    "# resample_sensor(df_hard, \"S117\", 60, method=\"median\")\n",
    "# → ValueError: Unsupported method 'median'. Choose from 'near', 'min', 'max', 'mean'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25499f6d-09e4-4fb4-9a90-b0460fcd5519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cbdbb-e618-4c4a-a9e2-d0d144bf1c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b61fd-e032-4c3c-963e-29e465736e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc527f-30c0-4221-bd42-0f2e2e583742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVA MARCO VECCHIA CONVERSIONE SENSORI A5 A9, ricopiata il codice può essere utile\n",
    "# --- OCCHIO --- questo ti pare che faccia la conversione del binario da dx verso sx, dovrebbe essere il contrario\n",
    "\n",
    "# def convert_a5_to_binary_bits(df,sensor):\n",
    "#     df_with_bits = df\n",
    "    \n",
    "#     for i in range(16):\n",
    "#         bit_position = 15 - i  # S1 = bit 15 (leftmost), S16 = bit 0 (rightmost)\n",
    "#         df_with_bits = df_with_bits.withColumn(\n",
    "#             f\"YOLO{i+1}\",\n",
    "#             (  col(sensor).bitwiseAND(1 << bit_position)  > 0 ).cast(\"int\")\n",
    "#         )\n",
    "    \n",
    "#     return df_with_bits\n",
    "\n",
    "# # Usage\n",
    "# df_with_binary = convert_a5_to_binary_bits(df_pivoted,\"A5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53382d-ff59-4872-b543-79315fdac955",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Anomaly detection (Prova Marco sarà da cancellare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3e04d-c6d4-4477-83ae-97221ac42da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_switch_anomaly(df, sensor, window_minutes=10, switch_threshold=5):\n",
    "    \"\"\"\n",
    "    Detects frequent switching for binary sensor, partitioned by day to avoid single-node pressure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract day (or hour) from timestamp to partition\n",
    "    df = df.withColumn(\"day\", to_date(from_unixtime(col(\"when\"))))\n",
    "\n",
    "    # Lag to get previous value within each partition --- OTHERWISE IT MOVES ALL IN THE SAME PARTITION\n",
    "    w_lag = Window.partitionBy(\"day\").orderBy(\"when\")\n",
    "    df_lagged = df.withColumn(\"prev\", lag(col(sensor)).over(w_lag))\n",
    "\n",
    "    # Detect change 0→1 or 1→0\n",
    "    df_changes = df_lagged.withColumn(\n",
    "        \"change\", when((col(\"prev\") != col(sensor)), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Rolling window over time with partition\n",
    "    w_time = Window.partitionBy(\"day\").orderBy(\"when\").rangeBetween(-window_minutes * 60, 0)\n",
    "    df_windowed = df_changes.withColumn(\n",
    "        \"change_count\", sum_(\"change\").over(w_time)\n",
    "    )\n",
    "\n",
    "    # Flag anomaly\n",
    "    df_anomaly = df_windowed.withColumn(\n",
    "        \"is_anomaly\", (col(\"change_count\") > switch_threshold)\n",
    "    ).drop(\"prev\", \"change\", \"change_count\", \"day\")\n",
    "\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849c25c-a75f-494b-b8ac-dcafe6e3ecf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_anom = detect_switch_anomaly(\n",
    "  df_resampled_S117,\n",
    "  sensor=\"S117\",\n",
    "  window_minutes=60,\n",
    "  switch_threshold=6\n",
    ")\n",
    "df_anom.filter(\"is_anomaly\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81b03a-aecd-46ae-aa92-d7ff6b6a7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = 1602882742-50*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ce942-07b8-49df-8664-b7dcba64f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine =1602882742+50*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979993d2-4313-4a1f-988c-156c1c6e7e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect(df_resampled_S117, sensors=[\"S117\"], start=ini, end=fine).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28753c-c068-4334-84de-1b6e141a9603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67fecb82-d909-4dc6-a026-16d5347faaa7",
   "metadata": {},
   "source": [
    "# Predictive Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b6c25-4344-4d39-b74f-886da19b1aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff72c85-a4cb-4486-adf4-4bb541f5e867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282937a-70c2-4cd6-a0f3-85864215978e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167f121-b1ef-4e2c-bf48-07f55a14afad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604ef5e-bbb2-4528-ab5c-b6901b18a4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a8d74-a326-4733-a3f7-157b2b1ad302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c34c12-7e6b-44c1-99be-3a8635e2a905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7593e0-7c05-4ff3-a3b8-6f0f6f24ce16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2930c16b-276b-4e9a-b7b2-78a9e82c0776",
   "metadata": {},
   "source": [
    "# *** Remember to close Spark Session ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33c131-fe4d-4135-8844-5d1015fb0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2f446-cd02-4d0d-a3d9-f501963a87be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fcced-1f35-4f53-b9bc-ba8e97617d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57265f1f-2d05-47e5-8427-099e79949cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
