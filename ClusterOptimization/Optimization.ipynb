{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9144ef0-c058-494d-8202-43ca4af588e0",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 3px solid #0a6d91; padding: 15px; display: flex; align-items: center; justify-content: space-between;\">\n",
    "\n",
    "  <!-- Left text -->\n",
    "  <div style=\"flex: 1; padding-right: 20px;\">\n",
    "    <h2 style= display: inline-block; padding: 5px 10px; border-radius: 3px;\">\n",
    "      MAPD (mod.B) Final Project\n",
    "    </h2>\n",
    "    <h3>Anomaly detection and <br>Predictive maintenance for\n",
    "industrial devices</h3>\n",
    "  </div>\n",
    "\n",
    "  <!-- Right images -->\n",
    "  <div style=\"flex: 0 0 auto; display: flex; align-items: center; gap: 20px;\">\n",
    "    <img src=\"https://th.bing.com/th/id/R.f158dd00f7e0e326ff081cf1acb39901?rik=tfJW%2frH3keCJ%2fg&riu=http%3a%2f%2fboostlab.dfa.unipd.it%2fimg%2flogo_pod.png&ehk=Th6GDiUuQTgD%2faBhIK7JUi15%2bG%2f35LzMJV9PFEPd9rg%3d&risl=&pid=ImgRaw&r=0\" alt=\"PoD\" width=\"250\"/>\n",
    "    <img src=\"https://www.unidformazione.com/wp-content/uploads/2018/04/unipd-universita-di-padova-1024x463.png\" alt=\"UNIPD\" width = \"350\" />\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "<div style=\"border-bottom: 3px solid #0a6d91\">\n",
    "    <p><strong>Authors</strong></p>\n",
    "    <ul>\n",
    "      <li>Boscolo Marco (2157559)</li>\n",
    "      <li>La Rovere Francesco (2164968)</li>\n",
    "      <li>Montagner Nicol√≤ (2165809)</li>\n",
    "      <li>Sabatini Raffaele (2165739)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5755b7a-361f-4d7f-8e85-78f51d47dba4",
   "metadata": {},
   "source": [
    "# Cluster parameters optimization\n",
    "\n",
    "We wish to study the behaviour of the parallelization in our code. In order to do so we apply all the transoformation to the dataset, timing each stage of the processing to compare different configurations, mainly the number of cores and the number of partitions used in the process. Since using all the dataset was a quite heavy task we decided to restrict to one hardware analysis. SW-088 was the hardware of choice. All the functions used in the following code are found in the file **Functions.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bb6fa4e-b08a-4776-9390-ffafa503d6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "from pyspark import StorageLevel\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Import all functions (improve readibility)\n",
    "from Functions import *\n",
    "\n",
    "# PySpark core\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (\n",
    "    coalesce,\n",
    "    col, lit, expr, when, count, sum as spark_sum, abs as spark_abs,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, row_number, desc, asc,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e42256-ad85-429f-a599-a2a82295ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = ['P1', 'P10', 'P15', 'P16', 'P17', 'P18', 'P2', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S100', 'S101', 'S102', 'S106', \n",
    "           'S107', 'S108', 'S109', 'S11', 'S110', 'S112', 'S113', 'S114', 'S115', 'S117', 'S118', 'S122', 'S123', 'S124', 'S125', 'S126', \n",
    "           'S127', 'S128', 'S129', 'S130', 'S137', 'S138', 'S140', 'S143', 'S147', 'S15', 'S151', 'S154', 'S157', 'S158', 'S159', 'S16', \n",
    "           'S163', 'S164', 'S165', 'S166', 'S167', 'S169', 'S17', 'S170', 'S171', 'S172', 'S173', 'S174', 'S175', 'S176', 'S178', 'S179', \n",
    "           'S180', 'S181', 'S183', 'S19', 'S2', 'S201', 'S202', 'S203', 'S204', 'S205', 'S206', 'S25', 'S3', 'S33', 'S34', 'S35', 'S37', \n",
    "           'S39', 'S40', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S5', 'S50', 'S53', 'S54', 'S55', 'S56', 'S57', 'S6', 'S63', \n",
    "           'S64', 'S69', 'S7', 'S70', 'S71', 'S72', 'S73', 'S8', 'S80', 'S81', 'S83', 'S86', 'S9', 'S90', 'S94', 'S97', 'SA1', 'SA10', \n",
    "           'SA11', 'SA12', 'SA2', 'SA3', 'SA4', 'SA5', 'SA6', 'SA7', 'SA8', 'SA9', 'SW']\n",
    "alarms = ['A5', 'A9', 'ComError']\n",
    "engines = [\"S117\", \"S118\", \"S169\", \"S170\"]\n",
    "list_hwid = ['SW-106', 'SW-065', 'SW-115', 'SW-088']\n",
    "\n",
    "\n",
    "\n",
    "frequency = 60\n",
    "nCores = [2, 4, 6, 8, 12, 14, 16]\n",
    "nPartitions = [2, 8, 16, 32, 64, 256]\n",
    "nExecutors = [2, 4, 6, 8] \n",
    "MEMexec = \"2200m\" \n",
    "\n",
    "loops = [1, 2, 3]\n",
    "for loop in loops:\n",
    "    OptimizationResults = {}\n",
    "    for core in nCores:\n",
    "        for partition in nPartitions:\n",
    "    \n",
    "            clear_output(wait=True)\n",
    "    \n",
    "            #----------------------CREATING DATAFRAME --------------------------\n",
    "            \n",
    "            TimeResults = {}\n",
    "            \n",
    "            #Create spark session\n",
    "            print(f'Creating Spark session for {(core, partition)}')\n",
    "            spark = CreateSparkSession(core, partition, MEMexec, log = False)\n",
    "    \n",
    "            print('Reading the CSV...')\n",
    "            startTime = time.time()\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\").repartition(partition, col(\"hwid\"))\n",
    "    \n",
    "            #Convert milliseconds into seconds\n",
    "            df = df.withColumn(\"when\", spark_round(col(\"when\") / 1000).cast(IntegerType()))\n",
    "            df.count()\n",
    "            endTime = time.time()\n",
    "    \n",
    "            TimeResults['LoadCSV'] = endTime - startTime\n",
    "            print('Load CSV time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "    \n",
    "    \n",
    "            #----------------------PREPROCESSING PIPELINE--------------------------\n",
    "    \n",
    "            \n",
    "            print('Pivot dataset...')\n",
    "    \n",
    "            startTime = time.time()\n",
    "            df = df.filter(col('hwid') == 'SW-088')\n",
    "            df_all_hw = (df.groupBy(\"hwid\", \"when\")\n",
    "                       .pivot(\"metric\")\n",
    "                       .agg(first(\"value\"))\n",
    "                       .withColumn(\"time\", from_unixtime(col(\"when\")))\n",
    "                       .orderBy(\"hwid\", \"when\"))\n",
    "    \n",
    "            print('Persist the dataframe...')\n",
    "            df_all_hw = df_all_hw.persist()\n",
    "            df_all_hw.count()\n",
    "            \n",
    "            endTime = time.time()\n",
    "    \n",
    "            TimeResults['Pivot'] = endTime - startTime\n",
    "            print('Pivot time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "    \n",
    "            \n",
    "            # Fill sensor gaps and build blocks of independent measurement\n",
    "            print('Starting preprocessing...')\n",
    "            \n",
    "            startTime = time.time()\n",
    "            #Create grid, homogeneous data\n",
    "            df_grid = CreateGrid(df_all_hw, interval=frequency)\n",
    "    \n",
    "            #Build independent blocks\n",
    "            df_blocks = BuildBlocks(df_grid, max_interval = 1800, sensors = sensors )\n",
    "    \n",
    "            #Fill the NULL values\n",
    "            df_blocks = FillNull(df_blocks, sensors + engines, max_gap=240)\n",
    "            \n",
    "            df_blocks = df_blocks.persist()\n",
    "            df_blocks.count()\n",
    "            \n",
    "            endTime = time.time()\n",
    "    \n",
    "            TimeResults['Preprocessing'] = endTime - startTime\n",
    "    \n",
    "            print('Preprocessing time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "    \n",
    "            df_all_hw.unpersist()\n",
    "    \n",
    "            #----------------------ANOMALY DETECTION--------------------------\n",
    "    \n",
    "            useless_sensors, useful_sensors = UsefulSensors(df_blocks, sensors)\n",
    "    \n",
    "            #Compute the anomalies for all the hardware sequentially (parallelized internally)\n",
    "            print('Starting Anomaly detection...')\n",
    "            startTime = time.time()\n",
    "    \n",
    "            df_anomalies = detect_anomalies(df = df_blocks, time_separator = 60*40, threshold = 8, sensors = engines, partition = partition).persist()\n",
    "            df_anomalies.count()\n",
    "    \n",
    "            endTime = time.time()\n",
    "    \n",
    "            TimeResults['AnomalyDetection'] = endTime - startTime\n",
    "    \n",
    "            print('Anomaly detection time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "    \n",
    "            #------------------------CORRELATIONS--------------------------\n",
    "            print('Starting computing Correlations...')\n",
    "    \n",
    "            startTime = time.time()\n",
    "    \n",
    "            #df_blocks = spark.createDataFrame(df_blocks.rdd, df_blocks.schema)\n",
    "            #df_anomalies = spark.createDataFrame(df_anomalies.rdd, df_anomalies.schema)\n",
    "            \n",
    "            joined_df = df_blocks.join(df_anomalies, on =[\"hwid\", 'when', *engines, 'BlockID'], how='left').persist()\n",
    "            joined_df.count()\n",
    "    \n",
    "            for i in list_hwid:\n",
    "                filter_hw = joined_df.filter(col('hwid') == i)\n",
    "                anomaly_corr = correlations(filter_hw, useful_sensors, 'flag_anomaly')\n",
    "                \n",
    "            endTime = time.time()\n",
    "    \n",
    "            TimeResults['Correlations'] = endTime - startTime\n",
    "            print('Correlation time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "    \n",
    "    \n",
    "            #------------------------PREDICTIVE MAINTEINANCE--------------------------\n",
    "    \n",
    "            print('Starting Predictive Mainteinance...')\n",
    "            startTime1 = time.time()\n",
    "    \n",
    "            df_alarms = extract_alarms(df_blocks.select(\"when\",\"A5\",\"A9\"), columns=[\"A5\", \"A9\"], bits=[6, 7, 8] ).persist()\n",
    "            df_alarms.count()\n",
    "    \n",
    "            endTime1 = time.time()\n",
    "            \n",
    "            list_df_final = {}\n",
    "            \n",
    "            df_final = joined_df.join(df_alarms.select('when', 'overheating') , on=['when'], how='left' )\n",
    "    \n",
    "            startTime2 = time.time()\n",
    "            \n",
    "            #Computing correlations\n",
    "            alarm_corr = correlations(df_final, useful_sensors, 'overheating')\n",
    "\n",
    "            df_final = df_final.filter(col(\"hwid\") == \"SW-088\")\n",
    "    \n",
    "            #Computing the predictive dataframe on overheating signals\n",
    "            target = \"overheating\"\n",
    "            df_final088 = add_predictive(df_final, target, window_before_heating=30, debug=False, join=True, partition = partition )\n",
    "            df_final088.count()\n",
    "    \n",
    "            endTime2 = time.time()\n",
    "    \n",
    "            TimeResults['Predictive Maintenance'] = (endTime1 - startTime1) + (endTime2 - startTime2)\n",
    "            print('Predictive Maintenance time: ', np.round((endTime1 - startTime1) + (endTime2 - startTime2), 2), ' seconds')\n",
    "    \n",
    "            #-----------------------------------Saving the results \n",
    "            \n",
    "            OptimizationResults[(core, partition)] = TimeResults\n",
    "    \n",
    "            data = []\n",
    "            for (cores, partition), metrics in OptimizationResults.items():\n",
    "                row = {'Resources': (cores, partition)}\n",
    "                row.update(metrics)\n",
    "                data.append(row)\n",
    "            \n",
    "            df_alt = pd.DataFrame(data)\n",
    "            df_alt.to_pickle(f'SC_4HW_{loop}.pkl')\n",
    "    \n",
    "            #-----------------------------------Clean up the memory\n",
    "    \n",
    "            try:\n",
    "                df_blocks.unpersist()\n",
    "                df_anomalies.unpersist()\n",
    "                df_alarms.unpersist()\n",
    "                \n",
    "                \n",
    "                #Garbage collection\n",
    "                gc.collect()\n",
    "                \n",
    "                #Clear Spark cache\n",
    "                spark.catalog.clearCache()\n",
    "                \n",
    "            except Exception as cleanup_error:\n",
    "                print(f\"Cleanup error: {cleanup_error}\")\n",
    "    \n",
    "            spark.stop()\n",
    "            time.sleep(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bc766-230c-4adc-bc65-8cbcea584739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
