{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb6fa4e-b08a-4776-9390-ffafa503d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#Import all functions (improve readibility)\n",
    "from Functions import *\n",
    "\n",
    "# PySpark core\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (\n",
    "    coalesce,\n",
    "    col, lit, expr, when, count, sum as spark_sum, abs as spark_abs,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, row_number, desc, asc,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50ec9a1-7117-469f-86a0-27ce111a2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session for (12, 10)\n",
      "Read the CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CSV time:  18.0 4\n",
      "Filtering hardware...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persist the filtered dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing time:  28.0 4\n",
      "Starting final persist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#nCore = np.arange(1, 17)\n",
    "#nPartitions = np.arange(2, 33, 2)\n",
    "nCores = [10, 12]\n",
    "nPartitions = [8, 10]\n",
    "frequency = 60\n",
    "\n",
    "OptimizationResults = {}\n",
    "\n",
    "\n",
    "for core in nCores:\n",
    "    for partitions in nPartitions:\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "        TimeResults = {}\n",
    "        \n",
    "        #Create spark session\n",
    "        print(f'Creating Spark session for {(core, partitions)}')\n",
    "        spark = CreateSparkSession(core)\n",
    "\n",
    "        print('Reading the CSV...')\n",
    "        startTime = time.time()\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\")\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['LoadCSV'] = endTime - startTime\n",
    "        print('Load CSV time: ', np.round(endTime - startTime, 3))\n",
    "\n",
    "        #Define the number of partitions\n",
    "        df = df.repartition(partitions)\n",
    "\n",
    "        #Convert milliseconds into seconds\n",
    "        df = df.withColumn(\"when\", spark_round(col(\"when\") / 1000).cast(IntegerType()))\n",
    "        \n",
    "        #Focus only on 1 hardware (conventional)\n",
    "        print('Filtering hardware...')\n",
    "        hardware = \"SW-106\"\n",
    "        df_hw = df  .filter(col(\"hwid\") == hardware)\\\n",
    "                    .groupBy(\"when\")\\\n",
    "                    .pivot(\"metric\")\\\n",
    "                    .agg(first(\"value\"))\\\n",
    "                    .withColumn(\"time\", from_unixtime(col(\"when\")))\\\n",
    "                    .orderBy(\"when\")\n",
    "\n",
    "        #Momentarily persist\n",
    "        print('Persist the filtered dataframe...')\n",
    "        df_hw = df_hw.persist()\n",
    "        #Trigge the persist through an action\n",
    "        df_hw.count()\n",
    "\n",
    "        \n",
    "        # Fill sensor gaps and build blocks of independent measurement\n",
    "        print('Starting preprocessing...')\n",
    "        startTime = time.time()\n",
    "        df_grid = FillGaps(df_hw, interval=frequency, modality=\"auto\")\n",
    "        df_final = BuildBlocks(df_grid, max_interval = 240)\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['Preprocessing'] = endTime - startTime\n",
    "\n",
    "        print('Preprocessing time: ', np.round(endTime - startTime, 3))\n",
    "        \n",
    "        #Persist and trigger the persist operation\n",
    "        print('Starting final persist...')\n",
    "        df_final = df_final.persist()\n",
    "        df_final.count()\n",
    "\n",
    "        OptimizationResults[(core, partitions)] = TimeResults\n",
    "\n",
    "\n",
    "        \n",
    "        spark.stop()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#------------- ANOMALY DETECTION NOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c237ca37-76ba-4d2d-b6da-3480695d19de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 8): {'LoadCSV': 20.984825611114502, 'Preprocessing': 34.580507040023804},\n",
       " (10, 10): {'LoadCSV': 19.71999764442444, 'Preprocessing': 40.66104340553284},\n",
       " (12, 8): {'LoadCSV': 18.766189575195312, 'Preprocessing': 28.407060623168945},\n",
       " (12, 10): {'LoadCSV': 18.23853588104248, 'Preprocessing': 28.279947996139526}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OptimizationResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
