{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb6fa4e-b08a-4776-9390-ffafa503d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Import all functions (improve readibility)\n",
    "from Functions import *\n",
    "\n",
    "# PySpark core\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (\n",
    "    coalesce,\n",
    "    col, lit, expr, when, count, sum as spark_sum, abs as spark_abs,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, row_number, desc, asc,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50ec9a1-7117-469f-86a0-27ce111a2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session for (12, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 07:17:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the CSV...\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o54.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mReading the CSV...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     60\u001b[39m startTime = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile:///mnt/shared/dataset.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.repartition(\u001b[32m4\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mhwid\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m#Convert milliseconds into seconds\u001b[39;00m\n\u001b[32m     64\u001b[39m df = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mwhen\u001b[39m\u001b[33m\"\u001b[39m, spark_round(col(\u001b[33m\"\u001b[39m\u001b[33mwhen\u001b[39m\u001b[33m\"\u001b[39m) / \u001b[32m1000\u001b[39m).cast(IntegerType()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling o54.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "#nCore = np.arange(1, 17)\n",
    "#nPartitions = np.arange(2, 33, 2)\n",
    "\n",
    "sensors = ['P1', 'P10', 'P15', 'P16', 'P17', 'P18', 'P2', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S100', 'S101', 'S102', 'S106', \n",
    "           'S107', 'S108', 'S109', 'S11', 'S110', 'S112', 'S113', 'S114', 'S115', 'S117', 'S118', 'S122', 'S123', 'S124', 'S125', 'S126', \n",
    "           'S127', 'S128', 'S129', 'S130', 'S137', 'S138', 'S140', 'S143', 'S147', 'S15', 'S151', 'S154', 'S157', 'S158', 'S159', 'S16', \n",
    "           'S163', 'S164', 'S165', 'S166', 'S167', 'S169', 'S17', 'S170', 'S171', 'S172', 'S173', 'S174', 'S175', 'S176', 'S178', 'S179', \n",
    "           'S180', 'S181', 'S183', 'S19', 'S2', 'S201', 'S202', 'S203', 'S204', 'S205', 'S206', 'S25', 'S3', 'S33', 'S34', 'S35', 'S37', \n",
    "           'S39', 'S40', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S5', 'S50', 'S53', 'S54', 'S55', 'S56', 'S57', 'S6', 'S63', \n",
    "           'S64', 'S69', 'S7', 'S70', 'S71', 'S72', 'S73', 'S8', 'S80', 'S81', 'S83', 'S86', 'S9', 'S90', 'S94', 'S97', 'SA1', 'SA10', \n",
    "           'SA11', 'SA12', 'SA2', 'SA3', 'SA4', 'SA5', 'SA6', 'SA7', 'SA8', 'SA9', 'SW']\n",
    "alarms = ['A5', 'A9', 'ComError']\n",
    "engines = [\"S117\", \"S118\", \"S169\", \"S170\"]\n",
    "frequency = 60\n",
    "OptimizationResults = {}\n",
    "\n",
    "\n",
    "\n",
    "#nCores = [4, 8, 12, 16]\n",
    "#nPartitions = [16, 32, 48, 64]\n",
    "\n",
    "nCores = [12]\n",
    "nPartitions = [64]\n",
    "\n",
    "Nexecutors = 4 \n",
    "MEMexec = \"2200m\" \n",
    "\n",
    "\n",
    "#Log file (for every warning, useful for warning 137)\n",
    "log_filename = \"WarninggLogs.txt\"\n",
    "with open(log_filename, 'w') as f:\n",
    "    f.write(f\"Spark Optimization Logs - {datetime.now()}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "for core in nCores:\n",
    "    for partition in nPartitions:\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        #----------------------CREATING DATAFRAME --------------------------\n",
    "\n",
    "        #Saving all the warning in a log file\n",
    "        with open(log_filename, 'a') as log_file:  \n",
    "            # Write configuration header\n",
    "            log_file.write(f\"Starting: {core} cores, {partition} partitions\\n\")\n",
    "            log_file.write(\"-\" * 50 + \"\\n\")\n",
    "            log_file.flush()\n",
    "            \n",
    "            with redirect_stderr(log_file):\n",
    "        \n",
    "        \n",
    "                TimeResults = {}\n",
    "                \n",
    "                #Create spark session\n",
    "                print(f'Creating Spark session for {(core, partition)}')\n",
    "                spark = CreateSparkSession(core, partition, Nexecutors, MEMexec, log = False)\n",
    "        \n",
    "                print('Reading the CSV...')\n",
    "                startTime = time.time()\n",
    "                df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\").repartition(4, col(\"hwid\"))\n",
    "        \n",
    "                #Convert milliseconds into seconds\n",
    "                df = df.withColumn(\"when\", spark_round(col(\"when\") / 1000).cast(IntegerType()))\n",
    "                df.show(3, truncate=False)\n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['LoadCSV'] = endTime - startTime\n",
    "                print('Load CSV time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "        \n",
    "                #----------------------PREPROCESSING PIPELINE--------------------------\n",
    "        \n",
    "                \n",
    "                #Focus only on 1 hardware (conventional)\n",
    "                print('Pivot dataset...')\n",
    "        \n",
    "                startTime = time.time()\n",
    "                #df_all_hw = Pivot(df)\n",
    "                df_all_hw = (df.groupBy(\"hwid\", \"when\")\n",
    "                           .pivot(\"metric\")\n",
    "                           .agg(first(\"value\"))\n",
    "                           .withColumn(\"time\", from_unixtime(col(\"when\")))\n",
    "                           .orderBy(\"hwid\", \"when\"))\n",
    "        \n",
    "                print('Persist the dataframe...')\n",
    "                df_all_hw = df_all_hw.persist()\n",
    "                df_all_hw.count()\n",
    "        \n",
    "                \n",
    "                #df_all_hw = df_all_hw.persist()\n",
    "                #df_all_hw.count()\n",
    "                \n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['Pivot'] = endTime - startTime\n",
    "                print('Pivot time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "                \n",
    "                # Fill sensor gaps and build blocks of independent measurement\n",
    "                print('Starting preprocessing...')\n",
    "                \n",
    "                startTime = time.time()\n",
    "                #Create grid, homogeneous data\n",
    "                df_grid = CreateGrid(df_all_hw, interval=frequency)\n",
    "        \n",
    "                #Build independent blocks\n",
    "                df_blocks = BuildBlocks(df_grid, max_interval = 1800, sensors = sensors )\n",
    "        \n",
    "                #Fill the NULL values\n",
    "                df_blocks = FillNull(df_blocks, sensors + engines, max_gap=240).persist()\n",
    "                df_blocks.count()\n",
    "                \n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['Preprocessing'] = endTime - startTime\n",
    "        \n",
    "                print('Preprocessing time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "                df_all_hw.unpersist()\n",
    "        \n",
    "                #----------------------ANOMALY DETECTION--------------------------\n",
    "        \n",
    "                useless_sensors, useful_sensors = UsefulSensors(df_blocks, sensors)\n",
    "                list_hw = df_blocks.select(\"hwid\").distinct().rdd.flatMap(lambda x : x).collect()\n",
    "        \n",
    "        \n",
    "                #Compute the anomalies for all the hardware sequentially (parallelized internally)\n",
    "                print('Starting Anomaly detection...')\n",
    "                startTime = time.time()\n",
    "                \n",
    "                list_df_anomalies = {}\n",
    "                for hw in list_hw:\n",
    "                    df_anomalies = detect_anomalies( \n",
    "                        df = df_blocks.filter( col(\"hwid\") == hw),\n",
    "                        time_separator = 60*40, # seconds\n",
    "                        threshold = 8, \n",
    "                        sensors = engines)\n",
    "        \n",
    "                    list_df_anomalies[hw] = df_anomalies\n",
    "        \n",
    "                #show the first 3 rows of anomaly record\n",
    "                list_df_anomalies['SW-106'].filter('flag_S117').show(3, truncate=False)\n",
    "        \n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['AnomalyDetection'] = endTime - startTime\n",
    "        \n",
    "                print('Anomaly detection time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "        \n",
    "                #------------------------CORRELATIONS--------------------------\n",
    "                print('Starting computing Correlations')\n",
    "                list_df_blocks_anom = {}\n",
    "        \n",
    "                startTime = time.time()\n",
    "                for hw in list_hw:\n",
    "                    df_anom = list_df_anomalies[hw].select('when', 'flag_anomaly')\n",
    "                    df_anom = spark.createDataFrame(df_anom.rdd, df_anom.schema)\n",
    "                    list_df_blocks_anom[hw] = df_blocks.filter( col(\"hwid\") == hw ).join( df_anom, on='when', how='left' )\n",
    "                    \n",
    "                    anomaly_corr = correlations(list_df_blocks_anom[hw], useful_sensors, 'flag_anomaly')\n",
    "                    top_anom_corr = anomaly_corr.head(3)['Sensors'].tolist()\n",
    "                    print(hw, top_anom_corr)\n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['Correlations'] = endTime - startTime\n",
    "        \n",
    "                print('Correlation time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "        \n",
    "                #------------------------PREDICTIVE MAINTEINANCE--------------------------\n",
    "        \n",
    "                print('Starting Predictive Mainteinance')\n",
    "        \n",
    "                startTime = time.time()\n",
    "        \n",
    "                list_df_final = {}\n",
    "                list_df_alarms = {}\n",
    "        \n",
    "        \n",
    "                for hw in list_hw:\n",
    "                    #Operations on the dataset to retrieve bits from A5 and A9 sensors\n",
    "                    \n",
    "                    df_alarms = extract_alarms(df_blocks.filter( col(\"hwid\") == hw).select(\"when\",\"A5\",\"A9\"),\n",
    "                                                columns=[\"A5\", \"A9\"], bits=[6, 7, 8] )\n",
    "        \n",
    "                    list_df_alarms[hw] = df_alarms\n",
    "                    print( f\"For hardware {hw} were recorded {df_alarms.agg(spark_sum(col('overheating'))).collect()[0][0]} overheating signals.\" )\n",
    "                    df_final = list_df_blocks_anom[hw].join(list_df_alarms[hw].select('when', 'overheating') , on='when', how='left' )\n",
    "                    list_df_final[hw] = df_final\n",
    "        \n",
    "                hw_to_inspect = 'SW-088'\n",
    "                df_final_088 = list_df_final['SW-088']\n",
    "                \n",
    "                #Computing correlations\n",
    "                alarm_corr = correlations(list_df_final[hw_to_inspect], useful_sensors, 'overheating')\n",
    "        \n",
    "                #Computing the predictive dataframe on overheating signals\n",
    "                target = \"overheating\"\n",
    "                df_final088 = add_predictive(df_final_088, target, window_before_heating=30, debug=False, join=True )\n",
    "        \n",
    "                df_final_088.count()\n",
    "        \n",
    "                endTime = time.time()\n",
    "        \n",
    "                TimeResults['Predictive Maintenance'] = endTime - startTime\n",
    "                print('Predictive Maintenance time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "                #-----------------------------------Saving the results \n",
    "                \n",
    "                OptimizationResults[(core, partition)] = TimeResults\n",
    "        \n",
    "                data = []\n",
    "                for (cores, partitions), metrics in OptimizationResults.items():\n",
    "                    row = {'Resources': (cores, partitions)}\n",
    "                    row.update(metrics)\n",
    "                    data.append(row)\n",
    "                \n",
    "                df_alt = pd.DataFrame(data)\n",
    "                df_alt.to_pickle('OptimizationResults.pkl')\n",
    "        \n",
    "        \n",
    "        \n",
    "                #-----------------------------------Clean up the memory\n",
    "        \n",
    "                try:\n",
    "                    df_blocks.unpersist()\n",
    "                    \n",
    "                    #Clear Python references\n",
    "                    for hw in list_hw:\n",
    "                        if hw in list_df_anomalies:\n",
    "                            del list_df_anomalies[hw]\n",
    "                        if hw in list_df_blocks_anom:\n",
    "                            del list_df_blocks_anom[hw]\n",
    "                        if hw in list_df_final:\n",
    "                            del list_df_final[hw]\n",
    "                        if hw in list_df_alarms:\n",
    "                            del list_df_alarms[hw]\n",
    "                    \n",
    "                    #Clear collections\n",
    "                    list_df_anomalies.clear()\n",
    "                    list_df_blocks_anom.clear()\n",
    "                    list_df_final.clear()\n",
    "                    list_df_alarms.clear()\n",
    "                    \n",
    "                    #Garbage collection\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    #Clear Spark cache\n",
    "                    spark.catalog.clearCache()\n",
    "                    \n",
    "                except Exception as cleanup_error:\n",
    "                    print(f\"Cleanup error: {cleanup_error}\")\n",
    "                    \n",
    "        \n",
    "                spark.stop()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237ca37-76ba-4d2d-b6da-3480695d19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizationResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c997398-c679-43b3-9104-00073e8485c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 07:18:03 WARN Dispatcher: Message RequestMessage(10.67.22.142:44850, NettyRpcEndpointRef(spark://CoarseGrainedScheduler@master:36485), StatusUpdate(0,0,FINISHED,org.apache.spark.util.SerializableBuffer@12c8e102,1,Map())) dropped due to sparkEnv is stopped. Could not find CoarseGrainedScheduler.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1a1ba-61c3-40b4-98bc-852dbddc1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('OptimizationResults.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
