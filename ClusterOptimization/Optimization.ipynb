{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb6fa4e-b08a-4776-9390-ffafa503d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#Import all functions (improve readibility)\n",
    "from Functions import *\n",
    "\n",
    "# PySpark core\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (\n",
    "    coalesce,\n",
    "    col, lit, expr, when, count, sum as spark_sum, abs as spark_abs,\n",
    "    round as spark_round, min as spark_min, max as spark_max, avg as spark_avg,\n",
    "    first, last, lag, row_number, desc, asc,\n",
    "    explode, sequence, from_unixtime, to_date, unix_timestamp,\n",
    "    window, min_by, mode, concat, monotonically_increasing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ec9a1-7117-469f-86a0-27ce111a2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session for (10, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/23 08:06:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+\n",
      "|when      |hwid  |metric|value|\n",
      "+----------+------+------+-----+\n",
      "|1601824322|SW-115|S117  |0    |\n",
      "|1601824322|SW-115|S115  |0    |\n",
      "|1601824322|SW-115|S114  |1    |\n",
      "+----------+------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Load CSV time:  41.54  seconds\n",
      "Pivot dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persist the dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=============================>                            (2 + 2) / 4]"
     ]
    }
   ],
   "source": [
    "#nCore = np.arange(1, 17)\n",
    "#nPartitions = np.arange(2, 33, 2)\n",
    "nCores = [10, 12]\n",
    "nPartitions = [8, 10]\n",
    "frequency = 60\n",
    "sensors = ['P1', 'P10', 'P15', 'P16', 'P17', 'P18', 'P2', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S100', 'S101', 'S102', 'S106', \n",
    "           'S107', 'S108', 'S109', 'S11', 'S110', 'S112', 'S113', 'S114', 'S115', 'S117', 'S118', 'S122', 'S123', 'S124', 'S125', 'S126', \n",
    "           'S127', 'S128', 'S129', 'S130', 'S137', 'S138', 'S140', 'S143', 'S147', 'S15', 'S151', 'S154', 'S157', 'S158', 'S159', 'S16', \n",
    "           'S163', 'S164', 'S165', 'S166', 'S167', 'S169', 'S17', 'S170', 'S171', 'S172', 'S173', 'S174', 'S175', 'S176', 'S178', 'S179', \n",
    "           'S180', 'S181', 'S183', 'S19', 'S2', 'S201', 'S202', 'S203', 'S204', 'S205', 'S206', 'S25', 'S3', 'S33', 'S34', 'S35', 'S37', \n",
    "           'S39', 'S40', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S5', 'S50', 'S53', 'S54', 'S55', 'S56', 'S57', 'S6', 'S63', \n",
    "           'S64', 'S69', 'S7', 'S70', 'S71', 'S72', 'S73', 'S8', 'S80', 'S81', 'S83', 'S86', 'S9', 'S90', 'S94', 'S97', 'SA1', 'SA10', \n",
    "           'SA11', 'SA12', 'SA2', 'SA3', 'SA4', 'SA5', 'SA6', 'SA7', 'SA8', 'SA9', 'SW']\n",
    "alarms = ['A5', 'A9', 'ComError']\n",
    "engines = [\"S117\", \"S118\", \"S169\", \"S170\"]\n",
    "\n",
    "OptimizationResults = {}\n",
    "\n",
    "\n",
    "for core in nCores:\n",
    "    for partitions in nPartitions:\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        #----------------------CREATING DATAFRAME --------------------------\n",
    "        \n",
    "        \n",
    "        TimeResults = {}\n",
    "        \n",
    "        #Create spark session\n",
    "        print(f'Creating Spark session for {(core, partitions)}')\n",
    "        spark = CreateSparkSession(core, partitions)\n",
    "\n",
    "        print('Reading the CSV...')\n",
    "        startTime = time.time()\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:///mnt/shared/dataset.csv\").repartition(4, col(\"hwid\"))\n",
    "\n",
    "        #Convert milliseconds into seconds\n",
    "        df = df.withColumn(\"when\", spark_round(col(\"when\") / 1000).cast(IntegerType()))\n",
    "        df.show(3, truncate=False)\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['LoadCSV'] = endTime - startTime\n",
    "        print('Load CSV time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "\n",
    "\n",
    "        #----------------------PREPROCESSING PIPELINE--------------------------\n",
    "\n",
    "        \n",
    "        #Focus only on 1 hardware (conventional)\n",
    "        print('Pivot dataset...')\n",
    "\n",
    "        startTime = time.time()\n",
    "        df_all_hw = Pivot(df)\n",
    "\n",
    "        print('Persist the dataframe...')\n",
    "        df_all_hw = df_all_hw.persist()\n",
    "        df_all_hw.select(\"when\",\"time\",*engines).orderBy(col(\"time\").asc()).show(5)\n",
    "        \n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['Pivot'] = endTime - startTime\n",
    "        print('Pivot time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "\n",
    "        \n",
    "        # Fill sensor gaps and build blocks of independent measurement\n",
    "        print('Starting preprocessing...')\n",
    "        \n",
    "        startTime = time.time()\n",
    "        #Create grid, homogeneous data\n",
    "        df_grid = CreateGrid(df_all_hw, interval=frequency)\n",
    "\n",
    "        #Build independent blocks\n",
    "        df_blocks = BuildBlocks(df_grid, max_interval = 1800, sensors = sensors )\n",
    "\n",
    "        #Fill the NULL values\n",
    "        df_blocks = FillNull(df_blocks, sensors + engines, max_gap=240).persist()\n",
    "        df_blocks.count()\n",
    "        \n",
    "        df_blocks.select(\"hwid\",\"BlockID\",\"when\",\"window_start\",\"window_end\",*engines,*alarms).show(3)\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['Preprocessing'] = endTime - startTime\n",
    "\n",
    "        print('Preprocessing time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "\n",
    "        df_all_hw.unpersist()\n",
    "\n",
    "\n",
    "        #----------------------ANOMALY DETECTION--------------------------\n",
    "\n",
    "        useless_sensors, useful_sensors = UsefulSensors(df_blocks, sensors)\n",
    "        list_hw = df_blocks.select(\"hwid\").distinct().rdd.flatMap(lambda x : x).collect()\n",
    "\n",
    "\n",
    "        #Compute the anomalies for all the hardware sequentially (parallelized internally)\n",
    "        print('Starting Anomaly detection...')\n",
    "        startTime = time.time()\n",
    "        \n",
    "        list_df_anomalies = {}\n",
    "        for hw in list_hw:\n",
    "            df_anomalies = detect_anomalies( \n",
    "                df = df_blocks.filter( col(\"hwid\") == hw),\n",
    "                time_separator = 60*40, # seconds\n",
    "                threshold = 8, \n",
    "                sensors = engines)\n",
    "\n",
    "            list_df_anomalies[hw] = df_anomalies\n",
    "\n",
    "        #show the first 3 rows of anomaly record\n",
    "        list_df_anomalies['SW-106'].filter('flag_S117').show(3, truncate=False)\n",
    "\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['AnomalyDetection'] = endTime - startTime\n",
    "\n",
    "\n",
    "        #-----------------CORRELATIONS----------------------\n",
    "        print('Starting computing Correlations')\n",
    "        list_df_blocks_anom = {}\n",
    "\n",
    "        startTime = time.time()\n",
    "        for hw in list_hw:\n",
    "            df_anom = list_df_anomalies[hw].select('when', 'flag_anomaly')\n",
    "            list_df_blocks_anom[hw] = df_blocks.filter( col(\"hwid\") == hw ).join( df_anom, on='when', how='left' )\n",
    "            \n",
    "            anomaly_corr = correlations(list_df_blocks_anom[hw], useful_sensors, 'flag_anomaly')\n",
    "            top_anom_corr = anomaly_corr.head(3)['Sensors'].tolist()\n",
    "            print(hw, top_anom_corr)\n",
    "        endTime = time.time()\n",
    "\n",
    "        TimeResults['Correlations'] = endTime - startTime\n",
    "\n",
    "        print('Correlation time: ', np.round(endTime - startTime, 2), ' seconds')\n",
    "\n",
    "\n",
    "    \n",
    "        OptimizationResults[(core, partitions)] = TimeResults\n",
    "\n",
    "\n",
    "        np.save('OptimizationResults.npy', OptimizationResults)\n",
    "\n",
    "\n",
    "        \n",
    "        spark.stop()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237ca37-76ba-4d2d-b6da-3480695d19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizationResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
